{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pickle, argparse, os, sys\n",
    "from sklearn.metrics import accuracy_score\n",
    "import numpy as np\n",
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.functional as F\n",
    "from collections import defaultdict\n",
    "import time\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.metrics import accuracy_score\n",
    "from helpers import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(training_file):\n",
    "    assert os.path.isfile(training_file), 'Training file does not exist'\n",
    "\n",
    "    # Your code starts here\n",
    "    # load the data, vocabulary, lookups and other things - all here in this function\n",
    "    sentences, voc, idx_2_word, word_2_idx, tag_mapper, target_size = get_training_data(training_file)\n",
    "    \n",
    "    # now load the embeddings\n",
    "    weights_matrix = get_glove('./glove.42B.300d.txt', 300, len(voc) + 1, word_2_idx) # +1 because of PAD token\n",
    "    \n",
    "    # maybe will be run on cpu\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    \n",
    "    # init the model\n",
    "    model = LSTMTagger(weights_matrix, 128, target_size)\n",
    "    model.apply(init_weights)\n",
    "    model = model.to(device)    \n",
    "    \n",
    "    # contruct the dataset and dataloader\n",
    "    ds = WSJDataset(sentences, word_2_idx, tag_mapper)\n",
    "    dl = DataLoader(ds, batch_size=128, collate_fn=collate_examples)\n",
    "\n",
    "    # cross-entropy and adam \n",
    "    criterion = torch.nn.CrossEntropyLoss(ignore_index=0) # ignore index is for target so should work\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "    \n",
    "    # train for 20 epochs\n",
    "    for epoch in range(40):\n",
    "        train_loss, train_acc = train_epoch(model, dl, optimizer, criterion, epoch)\n",
    "        \n",
    "    state_dict= model.state_dict()\n",
    "    to_serialize = {'state_dict': state_dict, 'word_2_idx': word_2_idx, 'tag_mapper': tag_mapper}\n",
    "    # Your code ends here\n",
    "    \n",
    "    return to_serialize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1807092 word vectors in glove.\n",
      "embed_matrix.shape (16927, 300)\n",
      "11689 words are found in glove\n",
      "Epoch: 001/010 | Batch 000/299 | Cost: 3.8495 | : Accuracy: 0.0206\n",
      "Epoch: 001/010 | Batch 100/299 | Cost: 3.4092 | : Accuracy: 0.4920\n",
      "Epoch: 001/010 | Batch 200/299 | Cost: 3.3003 | : Accuracy: 0.5906\n",
      "Epoch: 002/010 | Batch 000/299 | Cost: 3.2309 | : Accuracy: 0.6570\n",
      "Epoch: 002/010 | Batch 100/299 | Cost: 3.1930 | : Accuracy: 0.6928\n",
      "Epoch: 002/010 | Batch 200/299 | Cost: 3.1803 | : Accuracy: 0.7043\n",
      "Epoch: 003/010 | Batch 000/299 | Cost: 3.1771 | : Accuracy: 0.7072\n",
      "Epoch: 003/010 | Batch 100/299 | Cost: 3.1657 | : Accuracy: 0.7188\n",
      "Epoch: 003/010 | Batch 200/299 | Cost: 3.1553 | : Accuracy: 0.7289\n",
      "Epoch: 004/010 | Batch 000/299 | Cost: 3.0862 | : Accuracy: 0.8017\n",
      "Epoch: 004/010 | Batch 100/299 | Cost: 3.1080 | : Accuracy: 0.7774\n",
      "Epoch: 004/010 | Batch 200/299 | Cost: 3.0785 | : Accuracy: 0.8065\n",
      "Epoch: 005/010 | Batch 000/299 | Cost: 3.0718 | : Accuracy: 0.8135\n",
      "Epoch: 005/010 | Batch 100/299 | Cost: 3.0711 | : Accuracy: 0.8142\n",
      "Epoch: 005/010 | Batch 200/299 | Cost: 3.0488 | : Accuracy: 0.8411\n",
      "Epoch: 006/010 | Batch 000/299 | Cost: 3.0449 | : Accuracy: 0.8407\n",
      "Epoch: 006/010 | Batch 100/299 | Cost: 3.0504 | : Accuracy: 0.8347\n",
      "Epoch: 006/010 | Batch 200/299 | Cost: 3.0217 | : Accuracy: 0.8646\n",
      "Epoch: 007/010 | Batch 000/299 | Cost: 3.0318 | : Accuracy: 0.8534\n",
      "Epoch: 007/010 | Batch 100/299 | Cost: 3.0357 | : Accuracy: 0.8495\n",
      "Epoch: 007/010 | Batch 200/299 | Cost: 3.0178 | : Accuracy: 0.8675\n",
      "Epoch: 008/010 | Batch 000/299 | Cost: 3.0209 | : Accuracy: 0.8655\n",
      "Epoch: 008/010 | Batch 100/299 | Cost: 3.0131 | : Accuracy: 0.8725\n",
      "Epoch: 008/010 | Batch 200/299 | Cost: 2.9996 | : Accuracy: 0.8860\n",
      "Epoch: 009/010 | Batch 000/299 | Cost: 3.0130 | : Accuracy: 0.8731\n",
      "Epoch: 009/010 | Batch 100/299 | Cost: 3.0048 | : Accuracy: 0.8808\n",
      "Epoch: 009/010 | Batch 200/299 | Cost: 2.9957 | : Accuracy: 0.8899\n",
      "Epoch: 010/010 | Batch 000/299 | Cost: 3.0119 | : Accuracy: 0.8737\n",
      "Epoch: 010/010 | Batch 100/299 | Cost: 3.0047 | : Accuracy: 0.8808\n",
      "Epoch: 010/010 | Batch 200/299 | Cost: 2.9958 | : Accuracy: 0.8899\n",
      "Epoch: 011/010 | Batch 000/299 | Cost: 3.0051 | : Accuracy: 0.8807\n",
      "Epoch: 011/010 | Batch 100/299 | Cost: 2.9958 | : Accuracy: 0.8898\n",
      "Epoch: 011/010 | Batch 200/299 | Cost: 2.9861 | : Accuracy: 0.8999\n",
      "Epoch: 012/010 | Batch 000/299 | Cost: 3.0004 | : Accuracy: 0.8852\n",
      "Epoch: 012/010 | Batch 100/299 | Cost: 2.9946 | : Accuracy: 0.8911\n",
      "Epoch: 012/010 | Batch 200/299 | Cost: 2.9849 | : Accuracy: 0.9010\n",
      "Epoch: 013/010 | Batch 000/299 | Cost: 2.9998 | : Accuracy: 0.8855\n",
      "Epoch: 013/010 | Batch 100/299 | Cost: 2.9944 | : Accuracy: 0.8914\n",
      "Epoch: 013/010 | Batch 200/299 | Cost: 2.9844 | : Accuracy: 0.9013\n",
      "Epoch: 014/010 | Batch 000/299 | Cost: 2.9988 | : Accuracy: 0.8874\n",
      "Epoch: 014/010 | Batch 100/299 | Cost: 2.9944 | : Accuracy: 0.8914\n",
      "Epoch: 014/010 | Batch 200/299 | Cost: 2.9740 | : Accuracy: 0.9122\n",
      "Epoch: 015/010 | Batch 000/299 | Cost: 2.9875 | : Accuracy: 0.8988\n",
      "Epoch: 015/010 | Batch 100/299 | Cost: 2.9786 | : Accuracy: 0.9074\n",
      "Epoch: 015/010 | Batch 200/299 | Cost: 2.9653 | : Accuracy: 0.9207\n",
      "Epoch: 016/010 | Batch 000/299 | Cost: 2.9809 | : Accuracy: 0.9055\n",
      "Epoch: 016/010 | Batch 100/299 | Cost: 2.9675 | : Accuracy: 0.9183\n",
      "Epoch: 016/010 | Batch 200/299 | Cost: 2.9650 | : Accuracy: 0.9210\n",
      "Epoch: 017/010 | Batch 000/299 | Cost: 2.9805 | : Accuracy: 0.9061\n",
      "Epoch: 017/010 | Batch 100/299 | Cost: 2.9672 | : Accuracy: 0.9190\n",
      "Epoch: 017/010 | Batch 200/299 | Cost: 2.9641 | : Accuracy: 0.9219\n",
      "Epoch: 018/010 | Batch 000/299 | Cost: 2.9795 | : Accuracy: 0.9064\n",
      "Epoch: 018/010 | Batch 100/299 | Cost: 2.9667 | : Accuracy: 0.9193\n",
      "Epoch: 018/010 | Batch 200/299 | Cost: 2.9640 | : Accuracy: 0.9219\n",
      "Epoch: 019/010 | Batch 000/299 | Cost: 2.9794 | : Accuracy: 0.9067\n",
      "Epoch: 019/010 | Batch 100/299 | Cost: 2.9666 | : Accuracy: 0.9193\n",
      "Epoch: 019/010 | Batch 200/299 | Cost: 2.9578 | : Accuracy: 0.9283\n",
      "Epoch: 020/010 | Batch 000/299 | Cost: 2.9717 | : Accuracy: 0.9143\n",
      "Epoch: 020/010 | Batch 100/299 | Cost: 2.9623 | : Accuracy: 0.9241\n",
      "Epoch: 020/010 | Batch 200/299 | Cost: 2.9572 | : Accuracy: 0.9289\n",
      "Epoch: 021/010 | Batch 000/299 | Cost: 2.9677 | : Accuracy: 0.9185\n",
      "Epoch: 021/010 | Batch 100/299 | Cost: 2.9600 | : Accuracy: 0.9260\n",
      "Epoch: 021/010 | Batch 200/299 | Cost: 2.9548 | : Accuracy: 0.9313\n",
      "Epoch: 022/010 | Batch 000/299 | Cost: 2.9676 | : Accuracy: 0.9185\n",
      "Epoch: 022/010 | Batch 100/299 | Cost: 2.9588 | : Accuracy: 0.9273\n",
      "Epoch: 022/010 | Batch 200/299 | Cost: 2.9549 | : Accuracy: 0.9313\n",
      "Epoch: 023/010 | Batch 000/299 | Cost: 2.9678 | : Accuracy: 0.9181\n",
      "Epoch: 023/010 | Batch 100/299 | Cost: 2.9591 | : Accuracy: 0.9270\n",
      "Epoch: 023/010 | Batch 200/299 | Cost: 2.9545 | : Accuracy: 0.9316\n",
      "Epoch: 024/010 | Batch 000/299 | Cost: 2.9676 | : Accuracy: 0.9185\n",
      "Epoch: 024/010 | Batch 100/299 | Cost: 2.9589 | : Accuracy: 0.9273\n",
      "Epoch: 024/010 | Batch 200/299 | Cost: 2.9539 | : Accuracy: 0.9322\n",
      "Epoch: 025/010 | Batch 000/299 | Cost: 2.9676 | : Accuracy: 0.9185\n",
      "Epoch: 025/010 | Batch 100/299 | Cost: 2.9586 | : Accuracy: 0.9276\n",
      "Epoch: 025/010 | Batch 200/299 | Cost: 2.9543 | : Accuracy: 0.9319\n",
      "Epoch: 026/010 | Batch 000/299 | Cost: 2.9675 | : Accuracy: 0.9188\n",
      "Epoch: 026/010 | Batch 100/299 | Cost: 2.9587 | : Accuracy: 0.9273\n",
      "Epoch: 026/010 | Batch 200/299 | Cost: 2.9535 | : Accuracy: 0.9325\n",
      "Epoch: 027/010 | Batch 000/299 | Cost: 2.9673 | : Accuracy: 0.9188\n",
      "Epoch: 027/010 | Batch 100/299 | Cost: 2.9585 | : Accuracy: 0.9276\n",
      "Epoch: 027/010 | Batch 200/299 | Cost: 2.9529 | : Accuracy: 0.9330\n",
      "Epoch: 028/010 | Batch 000/299 | Cost: 2.9674 | : Accuracy: 0.9188\n",
      "Epoch: 028/010 | Batch 100/299 | Cost: 2.9584 | : Accuracy: 0.9276\n",
      "Epoch: 028/010 | Batch 200/299 | Cost: 2.9523 | : Accuracy: 0.9339\n",
      "Epoch: 029/010 | Batch 000/299 | Cost: 2.9670 | : Accuracy: 0.9191\n",
      "Epoch: 029/010 | Batch 100/299 | Cost: 2.9582 | : Accuracy: 0.9279\n",
      "Epoch: 029/010 | Batch 200/299 | Cost: 2.9521 | : Accuracy: 0.9339\n",
      "Epoch: 030/010 | Batch 000/299 | Cost: 2.9673 | : Accuracy: 0.9188\n",
      "Epoch: 030/010 | Batch 100/299 | Cost: 2.9584 | : Accuracy: 0.9276\n",
      "Epoch: 030/010 | Batch 200/299 | Cost: 2.9524 | : Accuracy: 0.9336\n",
      "Epoch: 031/010 | Batch 000/299 | Cost: 2.9675 | : Accuracy: 0.9185\n",
      "Epoch: 031/010 | Batch 100/299 | Cost: 2.9574 | : Accuracy: 0.9289\n",
      "Epoch: 031/010 | Batch 200/299 | Cost: 2.9523 | : Accuracy: 0.9339\n",
      "Epoch: 032/010 | Batch 000/299 | Cost: 2.9667 | : Accuracy: 0.9194\n",
      "Epoch: 032/010 | Batch 100/299 | Cost: 2.9570 | : Accuracy: 0.9292\n",
      "Epoch: 032/010 | Batch 200/299 | Cost: 2.9517 | : Accuracy: 0.9345\n",
      "Epoch: 033/010 | Batch 000/299 | Cost: 2.9665 | : Accuracy: 0.9197\n",
      "Epoch: 033/010 | Batch 100/299 | Cost: 2.9564 | : Accuracy: 0.9299\n",
      "Epoch: 033/010 | Batch 200/299 | Cost: 2.9511 | : Accuracy: 0.9351\n",
      "Epoch: 034/010 | Batch 000/299 | Cost: 2.9666 | : Accuracy: 0.9194\n",
      "Epoch: 034/010 | Batch 100/299 | Cost: 2.9556 | : Accuracy: 0.9305\n",
      "Epoch: 034/010 | Batch 200/299 | Cost: 2.9512 | : Accuracy: 0.9348\n",
      "Epoch: 035/010 | Batch 000/299 | Cost: 2.9664 | : Accuracy: 0.9197\n",
      "Epoch: 035/010 | Batch 100/299 | Cost: 2.9564 | : Accuracy: 0.9299\n",
      "Epoch: 035/010 | Batch 200/299 | Cost: 2.9514 | : Accuracy: 0.9345\n",
      "Epoch: 036/010 | Batch 000/299 | Cost: 2.9664 | : Accuracy: 0.9197\n",
      "Epoch: 036/010 | Batch 100/299 | Cost: 2.9565 | : Accuracy: 0.9295\n",
      "Epoch: 036/010 | Batch 200/299 | Cost: 2.9509 | : Accuracy: 0.9351\n",
      "Epoch: 037/010 | Batch 000/299 | Cost: 2.9660 | : Accuracy: 0.9201\n",
      "Epoch: 037/010 | Batch 100/299 | Cost: 2.9565 | : Accuracy: 0.9295\n",
      "Epoch: 037/010 | Batch 200/299 | Cost: 2.9506 | : Accuracy: 0.9357\n",
      "Epoch: 038/010 | Batch 000/299 | Cost: 2.9666 | : Accuracy: 0.9194\n",
      "Epoch: 038/010 | Batch 100/299 | Cost: 2.9566 | : Accuracy: 0.9295\n",
      "Epoch: 038/010 | Batch 200/299 | Cost: 2.9504 | : Accuracy: 0.9357\n",
      "Epoch: 039/010 | Batch 000/299 | Cost: 2.9661 | : Accuracy: 0.9201\n",
      "Epoch: 039/010 | Batch 100/299 | Cost: 2.9562 | : Accuracy: 0.9299\n",
      "Epoch: 039/010 | Batch 200/299 | Cost: 2.9499 | : Accuracy: 0.9363\n",
      "Epoch: 040/010 | Batch 000/299 | Cost: 2.9655 | : Accuracy: 0.9207\n",
      "Epoch: 040/010 | Batch 100/299 | Cost: 2.9562 | : Accuracy: 0.9299\n",
      "Epoch: 040/010 | Batch 200/299 | Cost: 2.9498 | : Accuracy: 0.9363\n"
     ]
    }
   ],
   "source": [
    "model = train('data/wsj1-18.training')\n",
    "torch.save(model, 'model.torch')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(model_file, data_file, label_file):\n",
    "    assert os.path.isfile(model_file), 'Model file does not exist'\n",
    "    assert os.path.isfile(data_file), 'Data file does not exist'\n",
    "    assert os.path.isfile(label_file), 'Label file does not exist'\n",
    "    \n",
    "    model_and_co = torch.load('model.torch')\n",
    "    state_dict = model_and_co['state_dict']\n",
    "    word_2_idx = model_and_co['word_2_idx']\n",
    "    tag_mapper = model_and_co['tag_mapper']\n",
    "    \n",
    "    # Your code starts here\n",
    "    valid_sentences = make_validation_pairs(data_file, label_file)\n",
    "    \n",
    "    valid_ds = WSJDataset(valid_sentences, word_2_idx, tag_mapper)\n",
    "    valid_dl = DataLoader(valid_ds, batch_size=32, collate_fn=collate_examples)\n",
    "    \n",
    "    model = LSTMTagger(np.zeros((len(word_2_idx), 300)), 128, len(tag_mapper)) # doesn't matter zeros because we load later\n",
    "    model.load_state_dict(state_dict)\n",
    "    model = model.to(device) \n",
    "    \n",
    "    # for the conviniency I get ground truth through the dataset as well\n",
    "    prediction, ground_truth = make_predictions(model, valid_dl)\n",
    "    prediction = np.concatenate(prediction)\n",
    "    ground_truth = np.concatenate(ground_truth)\n",
    "\n",
    "    print(f'The accuracy of the model is {100*accuracy_score(prediction, ground_truth):6.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The accuracy of the model is  89.04%\n"
     ]
    }
   ],
   "source": [
    "test('model.torch', 'data/wsj19-21.testing', 'data/wsj19-21.truth')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "panda",
   "language": "python",
   "name": "panda"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
