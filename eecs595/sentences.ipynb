{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn as nn\n",
    "\n",
    "from transformers.tokenization_utils_base import BatchEncoding\n",
    "from transformers.data.data_collator import default_data_collator\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn.functional as F\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 0xC0FFEE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "eat_with_folds = pd.read_pickle('./eat_with_folds.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "story         [Tom took watermelon out of the fridge., Tom c...\n",
       "label                                                         0\n",
       "breakpoint                                                    4\n",
       "id                                                   train_1041\n",
       "fold                                                          2\n",
       "Name: 1041, dtype: object"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eat_with_folds.iloc[-3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Tom took watermelon out of the fridge.',\n",
       " 'Tom cut the watermelon into cubes.',\n",
       " 'Tom put a banana in a cup.',\n",
       " 'Tom mashed the banana.',\n",
       " 'Tom cut the banana into slices.']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eat_with_folds.iloc[-3][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(seed)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "np.random.seed(seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentence encode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SentenceTransformer('roberta-large-nli-mean-tokens')\n",
    "model.cuda()\n",
    "all_embeddings = dict()\n",
    "\n",
    "for ix, row in eat_with_folds.iterrows():\n",
    "    all_embeddings[row['id']] = model.encode(row['story'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformers\n",
    "from transformers import (\n",
    "    AutoConfig,\n",
    "    AutoModelForSequenceClassification,\n",
    "    AutoModel,\n",
    "    AutoTokenizer,\n",
    "    default_data_collator,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name_or_path = 'roberta-large'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if BERT_MODEL.endswith('uncased'):\n",
    "#     DO_LOWER_CASE = True\n",
    "# elif BERT_MODEL.endswith('cased'):\n",
    "#     DO_LOWER_CASE = False\n",
    "# else:\n",
    "#     raise ValueError(\"Improper bert model name!\")\n",
    "\n",
    "# model_name = BERT_MODEL + \"_\" + str(abs(layer))\n",
    "\n",
    "# # tokenizer = BertTokenizer.from_pretrained(\n",
    "# #     BERT_MODEL,\n",
    "# #     do_lower_case=DO_LOWER_CASE,\n",
    "# #     never_split=(\"[UNK]\", \"[SEP]\", \"[PAD]\", \"[CLS]\", \"[MASK]\", \"[A]\", \"[B]\", \"[P]\")\n",
    "# # )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    model_name_or_path,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(text, tokenizer):\n",
    "    \"\"\"Returns a list of tokens and the positions of A, B, and the pronoun.\"\"\"\n",
    "    entries = {}\n",
    "    final_tokens = []\n",
    "    for token in tokenizer.tokenize(text):\n",
    "        if token in (\"[A]\", \"[B]\", \"[P]\"):\n",
    "            entries[token] = len(final_tokens)\n",
    "            continue\n",
    "        final_tokens.append(token)\n",
    "    return final_tokens #, (entries[\"[A]\"], entries[\"[B]\"], entries[\"[P]\"])\n",
    "\n",
    "class EATDataset(Dataset):\n",
    "    \"\"\"Custom EAT Dataset class\"\"\"\n",
    "\n",
    "    def __init__(self, df, tokenizer, all_embeddings, labeled=True, task=1):\n",
    "        self.labeled = labeled\n",
    "        if labeled:\n",
    "            if task == 1:\n",
    "                self.y = pd.get_dummies(df['label']).values.astype(\"bool\")\n",
    "            elif task == 2:\n",
    "                # TODO one-hot encode me\n",
    "                self.y = df['breakpoint'].values\n",
    "                \n",
    "        # Extracts the tokens and offsets(positions of A, B, and P)\n",
    "        self.offsets, self.tokens, self.in_urls, self.other_feats = [], [], [], []\n",
    "        \n",
    "        for ix, row in df.iterrows():\n",
    "            sentence_embeddings = all_embeddings[row['id']][:5, ]\n",
    "#             text = ' '.join(row['story']) \n",
    "#             self.tokens.append(tokenizer.encode(text, \n",
    "#                                 padding='max_length',\n",
    "#                                 max_length=100))\n",
    "            self.tokens.append(sentence_embeddings)\n",
    "#             tokens = tokenize(text, tokenizer)\n",
    "#             if len(tokens) <= 512:\n",
    "#                 self.tokens.append(tokenizer.convert_tokens_to_ids(\n",
    "#                     tokens))\n",
    "#             else:\n",
    "#                 self.tokens.append(tokenizer.convert_tokens_to_ids(\n",
    "#                     tokens[0:510]))\n",
    "#                 print('Shortened seq')\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.tokens)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if self.labeled:\n",
    "            return self.tokens[idx], self.y[idx]\n",
    "        return self.tokens[idx],  None\n",
    "    \n",
    "def collate_examples(batch, truncate_len=512):  # 512 as in paper\n",
    "    \"\"\"Batch preparation.\n",
    "    1. Pad the sequences\n",
    "    2. Transform the target.\n",
    "    \"\"\"\n",
    "    transposed = list(zip(*batch))\n",
    "    \n",
    "    max_len = min(\n",
    "        max((len(x) for x in transposed[0])),\n",
    "        truncate_len\n",
    "    )\n",
    "#     tokens = np.zeros((len(batch), max_len), dtype=np.int64)\n",
    "#     for i, row in enumerate(transposed[0]):\n",
    "#         row = np.array(row[:truncate_len])\n",
    "#         tokens[i, :len(row)] = row\n",
    "    token_tensor = torch.Tensor(transposed[0])\n",
    "\n",
    "    # Labels\n",
    "    if len(transposed) == 1:\n",
    "        return token_tensor, None\n",
    "    \n",
    "    one_hot_labels = torch.stack([\n",
    "        torch.from_numpy(x.astype(\"uint8\")) for x in transposed[-1]\n",
    "    ], dim=0)\n",
    "    \n",
    "    _, labels = one_hot_labels.max(dim=1)\n",
    "    \n",
    "    return token_tensor, labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch\n",
    "# import torch.nn as nn\n",
    "# from allennlp.modules.span_extractors import SelfAttentiveSpanExtractor\n",
    "# # from transformers.models.bert import BertModel\n",
    "# # from pytorch_pretrained_bert.modeling import BertModel\n",
    "\n",
    "\n",
    "# class Head(nn.Module):\n",
    "#     \"\"\"The MLP submodule\"\"\"\n",
    "\n",
    "#     def __init__(self, bert_hidden_size: int, cnn_context: int, hidden_size: int):\n",
    "#         super().__init__()\n",
    "#         self.bert_hidden_size = bert_hidden_size\n",
    "#         self.cnn_context = cnn_context\n",
    "#         self.proj_dim = 64\n",
    "#         self.k = 1 + 2 * self.cnn_context\n",
    "#         self.hidden_size = hidden_size\n",
    "\n",
    "# #         self.span_extractor = SelfAttentiveSpanExtractor(self.proj_dim)  # span extractor comes directly after BERT\n",
    "#         # all the main parameters are coming from the conv layer\n",
    "#         # works!\n",
    "#         self.context_conv = nn.Conv1d(self.bert_hidden_size, self.proj_dim, kernel_size=self.k, stride=1,\n",
    "#                                       padding=self.cnn_context, dilation=1, groups=1, bias=True)\n",
    "\n",
    "#         self.fc = nn.Sequential(\n",
    "#             nn.BatchNorm1d(32),\n",
    "#             #             nn.Dropout(0.7),\n",
    "#             nn.Linear(32, self.hidden_size),\n",
    "#             nn.ReLU(),\n",
    "#             nn.BatchNorm1d(self.hidden_size),\n",
    "#             nn.Dropout(0.6),\n",
    "#         )\n",
    "\n",
    "#         self.new_last_layer = nn.Linear(self.hidden_size, 2)\n",
    "#         self.lstm = nn.LSTM(self.bert_hidden_size, 32, 1, batch_first=True)\n",
    "        \n",
    "#         # 64 are from proj_dim, 2 are from url, 9 is for the other features, 1 is gender,\n",
    "#         # 3 are synt distance, 2 are the distances to the root\n",
    "\n",
    "#         # after fine-tuning BERT this is not required, throw away\n",
    "#         for i, module in enumerate(self.fc):\n",
    "#             if isinstance(module, (nn.BatchNorm1d, nn.BatchNorm2d)):\n",
    "#                 nn.init.constant_(module.weight, 1)\n",
    "#                 nn.init.constant_(module.bias, 0)\n",
    "#                 print(\"Initing batchnorm\")\n",
    "#             elif isinstance(module, nn.Linear):\n",
    "#                 if getattr(module, \"weight_v\", None) is not None:\n",
    "#                     nn.init.uniform_(module.weight_g, 0, 1)\n",
    "#                     nn.init.kaiming_normal_(module.weight_v)\n",
    "#                     print(\"Initing linear with weight normalization\")\n",
    "#                     assert model[i].weight_g is not None\n",
    "#                 else:\n",
    "#                     nn.init.kaiming_normal_(module.weight)\n",
    "#                     print(\"Initing linear\")\n",
    "#                 nn.init.constant_(module.bias, 0)\n",
    "\n",
    "#     def forward(self, bert_outputs):\n",
    "#         assert bert_outputs.size(2) == self.bert_hidden_size\n",
    "#         # reduce the dimension\n",
    "# #         conv_output = self.context_conv(bert_outputs.transpose(1, 2)).transpose(2, 1).contiguous()\n",
    "#         # and extract the span\n",
    "# #         maybe later?\n",
    "# #         extracted_outputs = self.span_extractor(conv_output, offsets).view(bert_outputs.size(0), -1)\n",
    "# #         extracted_outputs = bert_outputs.view(bert_outputs.size(0), -1)\n",
    "# #         print(outputs.shape)\n",
    "#         output, (hn, cn) = self.lstm(bert_outputs)\n",
    "        \n",
    "#         fc_output = self.fc(hn.squeeze(0))\n",
    "#         concatenated_outputs = torch.cat([fc_output], dim=1)\n",
    "#         return self.new_last_layer(concatenated_outputs)\n",
    "\n",
    "\n",
    "# class GAPModel(nn.Module):\n",
    "#     \"\"\"The main model.\"\"\"\n",
    "\n",
    "#     def __init__(self, bert_model: str, cnn_context: int, layer: int, hidden_size: int, device: torch.device):\n",
    "#         super().__init__()\n",
    "#         self.device = device\n",
    "#         if bert_model in (\"bert-base-uncased\", \"bert-base-cased\"):\n",
    "#             self.bert_hidden_size = 768\n",
    "#         elif bert_model in (\"bert-large-uncased\", \"bert-large-cased\"):\n",
    "#             self.bert_hidden_size = 1024\n",
    "#         else:\n",
    "#             raise ValueError(\"Unsupported BERT model.\")\n",
    "            \n",
    "#         config = AutoConfig.from_pretrained(\n",
    "#             bert_model,\n",
    "#             num_labels=2,\n",
    "#         )\n",
    "        \n",
    "#         self.bert = AutoModel.from_pretrained(\n",
    "#             bert_model,\n",
    "#             from_tf=False,\n",
    "#             config=config,\n",
    "#         )\n",
    "        \n",
    "#         self.num_layers = self.bert.config.num_hidden_layers\n",
    "#         self.head = Head(self.bert_hidden_size, cnn_context, hidden_size)\n",
    "\n",
    "#     def forward(self, token_tensor):\n",
    "# #         token_tensor = token_tensor.to(self.device)\n",
    "#         bert_outputs, _ = self.bert(\n",
    "#             token_tensor, attention_mask=(token_tensor > 0).long(),\n",
    "#             token_type_ids=None)\n",
    "    \n",
    "#         # calling output_all_encoded_layers False and True with last index is different\n",
    "#         # most likely because of the pooling layer. Without pooling layers slighly better results\n",
    "#         #         h_enc = bert_outputs[-1]\n",
    "#         #         h_lex = self.bert.embeddings.word_embeddings(token_tensor) # this option takes the first part of bert only\n",
    "#         #         h_lex = self.bert.embeddings.LayerNorm(h_lex)\n",
    "        \n",
    "#         #         bert_outputs = bert_outputs # bert_outputs[self.layer]\n",
    "\n",
    "#         head_outputs = self.head(bert_outputs)\n",
    "#         return head_outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = GAPModel(\"bert-base-uncased\", 0, 12345, 256, 'vlavla')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def children(m):\n",
    "    return m if isinstance(m, (list, tuple)) else list(m.children())\n",
    "\n",
    "\n",
    "def set_trainable_attr(m, b):\n",
    "    m.trainable = b\n",
    "    for p in m.parameters():\n",
    "        p.requires_grad = b\n",
    "\n",
    "\n",
    "def apply_leaf(m, f):\n",
    "    c = children(m)\n",
    "    if isinstance(m, nn.Module):\n",
    "        f(m)\n",
    "    if len(c) > 0:\n",
    "        for l in c:\n",
    "            apply_leaf(l, f)\n",
    "\n",
    "\n",
    "def set_trainable(l, b):\n",
    "    apply_leaf(l, lambda m: set_trainable_attr(m, b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_name_or_path = 't5-small'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers.modeling_roberta import RobertaPreTrainedModel, RobertaModel, RobertaClassificationHead"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RobertaForSequenceClassification(RobertaPreTrainedModel):\n",
    "    _keys_to_ignore_on_load_missing = [r\"position_ids\"]\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super().__init__(config)\n",
    "        self.num_labels = config.num_labels\n",
    "\n",
    "        self.roberta = RobertaModel(config, add_pooling_layer=False)\n",
    "        self.classifier = RobertaClassificationHead(config)\n",
    "\n",
    "        self.init_weights()\n",
    "        \n",
    "    def forward(\n",
    "        self,\n",
    "        input_ids=None,\n",
    "        attention_mask=None,\n",
    "        token_type_ids=None,\n",
    "        position_ids=None,\n",
    "        head_mask=None,\n",
    "        inputs_embeds=None,\n",
    "        labels=None,\n",
    "        output_attentions=None,\n",
    "        output_hidden_states=None,\n",
    "        return_dict=None,\n",
    "    ):\n",
    "        r\"\"\"\n",
    "        labels (:obj:`torch.LongTensor` of shape :obj:`(batch_size,)`, `optional`):\n",
    "            Labels for computing the sequence classification/regression loss. Indices should be in :obj:`[0, ...,\n",
    "            config.num_labels - 1]`. If :obj:`config.num_labels == 1` a regression loss is computed (Mean-Square loss),\n",
    "            If :obj:`config.num_labels > 1` a classification loss is computed (Cross-Entropy).\n",
    "        \"\"\"\n",
    "        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n",
    "\n",
    "#         outputs = self.roberta(\n",
    "#             input_ids,\n",
    "#             attention_mask=attention_mask,\n",
    "#             token_type_ids=token_type_ids,\n",
    "#             position_ids=position_ids,\n",
    "#             head_mask=head_mask,\n",
    "#             inputs_embeds=inputs_embeds,\n",
    "#             output_attentions=output_attentions,\n",
    "#             output_hidden_states=output_hidden_states,\n",
    "#             return_dict=return_dict,\n",
    "#         )\n",
    "        sequence_output = input_ids # outputs[0]\n",
    "        logits = self.classifier(sequence_output)\n",
    "\n",
    "        loss = None\n",
    "        if labels is not None:\n",
    "            if self.num_labels == 1:\n",
    "                #  We are doing regression\n",
    "                loss_fct = MSELoss()\n",
    "                loss = loss_fct(logits.view(-1), labels.view(-1))\n",
    "            else:\n",
    "                loss_fct = CrossEntropyLoss()\n",
    "                loss = loss_fct(logits.view(-1, self.num_labels), labels.view(-1))\n",
    "\n",
    "        if not return_dict:\n",
    "            output = (logits,) + ()\n",
    "            return ((loss,) + output) if loss is not None else output\n",
    "\n",
    "        return SequenceClassifierOutput(\n",
    "            loss=loss,\n",
    "            logits=logits,\n",
    "            hidden_states=outputs.hidden_states,\n",
    "            attentions=outputs.attentions,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "config = AutoConfig.from_pretrained(\n",
    "    model_name_or_path,\n",
    "    num_labels=2,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at roberta-large were not used when initializing RobertaForSequenceClassification: ['lm_head.bias', 'lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight', 'roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.embeddings.position_ids', 'classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.out_proj.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model = RobertaForSequenceClassification.from_pretrained(\n",
    "    model_name_or_path,\n",
    "    config=config,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model.cuda();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "set_trainable(model.roberta, False)\n",
    "set_trainable(model.classifier, True)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "pytorch_total_params = sum(p.numel() for p in model.parameters())\n",
    "pytorch_trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "355361794"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pytorch_total_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1051650"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pytorch_trainable_params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training helper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "from train_helpers import GAPBot, TriangularLR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fold n#0\n"
     ]
    }
   ],
   "source": [
    "for fold in range(10):\n",
    "    print(\"fold n#{}\".format(fold))\n",
    "    train = eat_with_folds[eat_with_folds['fold'] != fold]\n",
    "    val = eat_with_folds[eat_with_folds['fold'] == fold]\n",
    "    \n",
    "    train_ds = EATDataset(train, tokenizer, all_embeddings)\n",
    "    valid_ds = EATDataset(val, tokenizer, all_embeddings)\n",
    "    \n",
    "    train_loader = DataLoader(\n",
    "        train_ds,\n",
    "        collate_fn=collate_examples,\n",
    "        batch_size=20,\n",
    "        num_workers=2,\n",
    "        pin_memory=True,\n",
    "        shuffle=False,\n",
    "        drop_last=False\n",
    "    )\n",
    "\n",
    "    val_loader = DataLoader(\n",
    "        valid_ds,\n",
    "        collate_fn=collate_examples,\n",
    "        batch_size=64,\n",
    "        num_workers=2,\n",
    "        pin_memory=True,\n",
    "        shuffle=False\n",
    "    )\n",
    "    \n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = iter(train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "b = next(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([20, 5, 1024])"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([20, 2])"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model(b[0].cuda())[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = torch.nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "next(a)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.roberta((next(a)[0]).cuda())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model.roberta((next(a)[0]).cuda())[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "criterion(model((next(a)[0]).cuda())[0], next(a)[1].cuda())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.convert_ids_to_tokens(15691)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ####################### TRAIN ######################\n",
    "# # Initialize our Trainer\n",
    "# def compute_metrics(pred: EvalPrediction):\n",
    "#     labels = pred.label_ids\n",
    "#     preds = pred.predictions.argmax(-1)\n",
    "#     precision, recall, f1, _ = precision_recall_fscore_support(labels, preds, average='macro')\n",
    "#     acc = accuracy_score(labels, preds)\n",
    "#     return {\n",
    "#         'accuracy': acc,\n",
    "#         'f1': f1,\n",
    "#         'precision': precision,\n",
    "#         'recall': recall\n",
    "#     }\n",
    "\n",
    "# training_args = TrainingArguments(output_dir='./',\n",
    "#                                 overwrite_output_dir=True, \n",
    "#                                 do_train=True, \n",
    "#                                 do_eval=True,\n",
    "#                                 evaluation_strategy='epoch',\n",
    "#                                 per_device_train_batch_size=10,\n",
    "#                                 per_device_eval_batch_size=8,\n",
    "#                                 gradient_accumulation_steps=1,\n",
    "#                                 learning_rate=2e-06,\n",
    "#                                 weight_decay=0.0, \n",
    "#                                 adam_beta1=0.9, \n",
    "#                                 adam_beta2=0.999, \n",
    "#                                 adam_epsilon=1e-08, \n",
    "#                                 max_grad_norm=1.0, \n",
    "#                                 num_train_epochs=20.0,\n",
    "#                                 max_steps=-1, \n",
    "#                                 warmup_steps=0,\n",
    "#                                 logging_dir='runs/whataver', \n",
    "#                                 logging_first_step=False, \n",
    "#                                 logging_steps=100, \n",
    "#                                 save_steps=500,\n",
    "#                                 seed=42, \n",
    "#                                 eval_steps=100,\n",
    "#                                 dataloader_num_workers=0)\n",
    "\n",
    "# trainer = Trainer(\n",
    "#     model=model,\n",
    "#     args=training_args,\n",
    "#     train_dataset=train_ds,\n",
    "#     eval_dataset=valid_ds,\n",
    "#     compute_metrics=compute_metrics,\n",
    "#     tokenizer=tokenizer,\n",
    "#     # Data collator will default to DataCollatorWithPadding, so we change it if we already did the padding.\n",
    "#     data_collator=default_data_collator,\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[[11/26/2020 02:24:08 PM]] SEED: 2711\n",
      "[[11/26/2020 02:24:08 PM]] # of paramters: 355,361,794\n",
      "[[11/26/2020 02:24:08 PM]] # of trainable paramters: 1,051,650\n"
     ]
    }
   ],
   "source": [
    "# TODO: no need to create each time separate folder, but different checkpoints\n",
    "bot = GAPBot(\n",
    "    model, train_loader, val_loader,\n",
    "    optimizer=optimizer, echo=True,\n",
    "    avg_window=25, checkpoint_dir=\"./models/gap/\" + 'try' + '/' + str(fold) + '/'\n",
    ")\n",
    "\n",
    "steps_per_epoch = len(train_loader) * 2\n",
    "n_steps = steps_per_epoch * 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[[11/26/2020 02:24:09 PM]] Optimizer Adam (\n",
      "Parameter Group 0\n",
      "    amsgrad: False\n",
      "    betas: (0.9, 0.999)\n",
      "    eps: 1e-08\n",
      "    initial_lr: 1e-05\n",
      "    lr: 5.000000000000001e-07\n",
      "    weight_decay: 0\n",
      ")\n",
      "[[11/26/2020 02:24:09 PM]] Batches per epoch: 47\n",
      "[[11/26/2020 02:24:09 PM]] ====================Epoch 1====================\n",
      "[[11/26/2020 02:24:09 PM]] Step 20: train 0.645113 lr: 1.657e-06\n",
      "[[11/26/2020 02:24:10 PM]] Step 40: train 0.641647 lr: 2.875e-06\n",
      "[[11/26/2020 02:24:10 PM]] ====================Epoch 2====================\n",
      "[[11/26/2020 02:24:10 PM]] Step 60: train 0.647229 lr: 4.093e-06\n",
      "[[11/26/2020 02:24:10 PM]] Step 80: train 0.649033 lr: 5.311e-06\n",
      "[[11/26/2020 02:24:11 PM]] ====================Epoch 3====================\n",
      "[[11/26/2020 02:24:11 PM]] Step 100: train 0.647806 lr: 6.529e-06\n",
      "[[11/26/2020 02:24:11 PM]] Step 120: train 0.644060 lr: 7.747e-06\n",
      "[[11/26/2020 02:24:11 PM]] Step 140: train 0.644504 lr: 8.965e-06\n",
      "[[11/26/2020 02:24:12 PM]] ====================Epoch 4====================\n",
      "[[11/26/2020 02:24:12 PM]] Step 160: train 0.643391 lr: 9.909e-06\n",
      "[[11/26/2020 02:24:12 PM]] Step 180: train 0.643309 lr: 9.304e-06\n",
      "[[11/26/2020 02:24:12 PM]] ====================Epoch 5====================\n",
      "[[11/26/2020 02:24:13 PM]] Step 200: train 0.643833 lr: 8.699e-06\n",
      "100%|██████████| 2/2 [00:00<00:00,  3.08it/s]\n",
      "[[11/26/2020 02:24:14 PM]] Snapshot loss 1.016435\n",
      "[[11/26/2020 02:24:17 PM]] Saving checkpoint models/gap/try/0/best.pth...\n",
      "[[11/26/2020 02:24:17 PM]] New low\n",
      "\n",
      "[[11/26/2020 02:24:18 PM]] Step 220: train 0.644555 lr: 8.094e-06\n",
      "[[11/26/2020 02:24:18 PM]] ====================Epoch 6====================\n",
      "[[11/26/2020 02:24:18 PM]] Step 240: train 0.644719 lr: 7.489e-06\n",
      "[[11/26/2020 02:24:18 PM]] Step 260: train 0.644612 lr: 6.884e-06\n",
      "[[11/26/2020 02:24:18 PM]] Step 280: train 0.644064 lr: 6.279e-06\n",
      "[[11/26/2020 02:24:19 PM]] ====================Epoch 7====================\n",
      "[[11/26/2020 02:24:19 PM]] Step 300: train 0.643573 lr: 5.674e-06\n",
      "[[11/26/2020 02:24:19 PM]] Step 320: train 0.643460 lr: 5.068e-06\n",
      "[[11/26/2020 02:24:19 PM]] ====================Epoch 8====================\n",
      "Process Process-126:\n",
      "Traceback (most recent call last):\n",
      "KeyboardInterrupt\n",
      "  File \"/home/amabza/anaconda3/envs/eecs595/lib/python3.6/multiprocessing/process.py\", line 251, in _bootstrap\n",
      "    util._run_after_forkers()\n",
      "Exception ignored in: <bound method _MultiProcessingDataLoaderIter.__del__ of <torch.utils.data.dataloader._MultiProcessingDataLoaderIter object at 0x7f0d53f5fcc0>>\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/amabza/anaconda3/envs/eecs595/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 1203, in __del__\n",
      "    self._shutdown_workers()\n",
      "  File \"/home/amabza/anaconda3/envs/eecs595/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 1174, in _shutdown_workers\n",
      "    if self._persistent_workers or self._workers_status[worker_id]:\n",
      "AttributeError: '_MultiProcessingDataLoaderIter' object has no attribute '_workers_status'\n",
      "  File \"/home/amabza/anaconda3/envs/eecs595/lib/python3.6/multiprocessing/util.py\", line 132, in _run_after_forkers\n",
      "    func(obj)\n",
      "  File \"/home/amabza/anaconda3/envs/eecs595/lib/python3.6/site-packages/torch/multiprocessing/_atfork.py\", line 10, in wrapper\n",
      "    func()\n"
     ]
    }
   ],
   "source": [
    "bot.train(\n",
    "    3000,\n",
    "    log_interval=20,\n",
    "    snapshot_interval=200,  # check the performance every epoch\n",
    "    scheduler=TriangularLR(\n",
    "        optimizer, 20, ratio=2, steps_per_cycle=n_steps)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the best checkpoint\n",
    "bot.load_model(bot.best_performers[0][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0,\n",
       "        1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1,\n",
       "        0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0,\n",
       "        1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0,\n",
       "        1, 0, 0, 1, 1, 1, 0, 1])"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1,\n",
       "        0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0,\n",
       "        1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1,\n",
       "        1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1,\n",
       "        0, 1, 0, 1, 0, 0, 1, 0])"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred[0].argmax(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:00<00:00,  3.11it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'accuracy': 0.17307692307692307, 'f1': 0.17307692307692307, 'precision': 0.17314095449500555, 'recall': 0.17314095449500555}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "pred = bot.predict(val_loader, return_y=True)\n",
    "\n",
    "precision, recall, f1, _ = precision_recall_fscore_support(pred[1], pred[0].argmax(axis=1), average='macro')\n",
    "acc = accuracy_score(pred[1], pred[0].argmax(axis=1))\n",
    "\n",
    "metrics = {'accuracy': acc,\n",
    "            'f1': f1,\n",
    "            'precision': precision,\n",
    "            'recall': recall}\n",
    "\n",
    "print(metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# peek into swag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import glob\n",
    "import json\n",
    "import logging\n",
    "import os\n",
    "from dataclasses import dataclass\n",
    "from enum import Enum\n",
    "from typing import List, Optional\n",
    "\n",
    "import tqdm\n",
    "from filelock import FileLock\n",
    "\n",
    "from transformers import PreTrainedTokenizer, is_tf_available, is_torch_available\n",
    "\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "\n",
    "@dataclass(frozen=True)\n",
    "class InputExample:\n",
    "    \"\"\"\n",
    "    A single training/test example for multiple choice\n",
    "    Args:\n",
    "        example_id: Unique id for the example.\n",
    "        question: string. The untokenized text of the second sequence (question).\n",
    "        contexts: list of str. The untokenized text of the first sequence (context of corresponding question).\n",
    "        endings: list of str. multiple choice's options. Its length must be equal to contexts' length.\n",
    "        label: (Optional) string. The label of the example. This should be\n",
    "        specified for train and dev examples, but not for test examples.\n",
    "    \"\"\"\n",
    "\n",
    "    example_id: str\n",
    "    question: str\n",
    "    contexts: List[str]\n",
    "    endings: List[str]\n",
    "    label: Optional[str]\n",
    "\n",
    "\n",
    "@dataclass(frozen=True)\n",
    "class InputFeatures:\n",
    "    \"\"\"\n",
    "    A single set of features of data.\n",
    "    Property names are the same names as the corresponding inputs to a model.\n",
    "    \"\"\"\n",
    "\n",
    "    example_id: str\n",
    "    input_ids: List[List[int]]\n",
    "    attention_mask: Optional[List[List[int]]]\n",
    "    token_type_ids: Optional[List[List[int]]]\n",
    "    label: Optional[int]\n",
    "\n",
    "\n",
    "class Split(Enum):\n",
    "    train = \"train\"\n",
    "    dev = \"dev\"\n",
    "    test = \"test\"\n",
    "\n",
    "\n",
    "if is_torch_available():\n",
    "    import torch\n",
    "    from torch.utils.data.dataset import Dataset\n",
    "\n",
    "    class MultipleChoiceDataset(Dataset):\n",
    "        \"\"\"\n",
    "        This will be superseded by a framework-agnostic approach\n",
    "        soon.\n",
    "        \"\"\"\n",
    "\n",
    "        features: List[InputFeatures]\n",
    "\n",
    "        def __init__(\n",
    "            self,\n",
    "            data_dir: str,\n",
    "            tokenizer: PreTrainedTokenizer,\n",
    "            task: str,\n",
    "            max_seq_length: Optional[int] = None,\n",
    "            overwrite_cache=False,\n",
    "            mode: Split = Split.train,\n",
    "        ):\n",
    "            processor = processors[task]()\n",
    "\n",
    "            cached_features_file = os.path.join(\n",
    "                data_dir,\n",
    "                \"cached_{}_{}_{}_{}\".format(mode.value, tokenizer.__class__.__name__, str(max_seq_length), task,),\n",
    "            )\n",
    "\n",
    "            # Make sure only the first process in distributed training processes the dataset,\n",
    "            # and the others will use the cache.\n",
    "            lock_path = cached_features_file + \".lock\"\n",
    "            with FileLock(lock_path):\n",
    "\n",
    "                if os.path.exists(cached_features_file) and not overwrite_cache:\n",
    "                    logger.info(f\"Loading features from cached file {cached_features_file}\")\n",
    "                    self.features = torch.load(cached_features_file)\n",
    "                else:\n",
    "                    logger.info(f\"Creating features from dataset file at {data_dir}\")\n",
    "                    label_list = processor.get_labels()\n",
    "                    if mode == Split.dev:\n",
    "                        examples = processor.get_dev_examples(data_dir)\n",
    "                    elif mode == Split.test:\n",
    "                        examples = processor.get_test_examples(data_dir)\n",
    "                    else:\n",
    "                        examples = processor.get_train_examples(data_dir)\n",
    "                    logger.info(\"Training examples: %s\", len(examples))\n",
    "                    # TODO clean up all this to leverage built-in features of tokenizers\n",
    "                    self.features = convert_examples_to_features(\n",
    "                        examples,\n",
    "                        label_list,\n",
    "                        max_seq_length,\n",
    "                        tokenizer,\n",
    "                        pad_on_left=bool(tokenizer.padding_side == \"left\"),\n",
    "                        pad_token=tokenizer.pad_token_id,\n",
    "                        pad_token_segment_id=tokenizer.pad_token_type_id,\n",
    "                    )\n",
    "                    logger.info(\"Saving features into cached file %s\", cached_features_file)\n",
    "                    torch.save(self.features, cached_features_file)\n",
    "\n",
    "        def __len__(self):\n",
    "            return len(self.features)\n",
    "\n",
    "        def __getitem__(self, i) -> InputFeatures:\n",
    "            return self.features[i]\n",
    "\n",
    "\n",
    "if is_tf_available():\n",
    "    import tensorflow as tf\n",
    "\n",
    "    class TFMultipleChoiceDataset:\n",
    "        \"\"\"\n",
    "        This will be superseded by a framework-agnostic approach\n",
    "        soon.\n",
    "        \"\"\"\n",
    "\n",
    "        features: List[InputFeatures]\n",
    "\n",
    "        def __init__(\n",
    "            self,\n",
    "            data_dir: str,\n",
    "            tokenizer: PreTrainedTokenizer,\n",
    "            task: str,\n",
    "            max_seq_length: Optional[int] = 128,\n",
    "            overwrite_cache=False,\n",
    "            mode: Split = Split.train,\n",
    "        ):\n",
    "            processor = processors[task]()\n",
    "\n",
    "            logger.info(f\"Creating features from dataset file at {data_dir}\")\n",
    "            label_list = processor.get_labels()\n",
    "            if mode == Split.dev:\n",
    "                examples = processor.get_dev_examples(data_dir)\n",
    "            elif mode == Split.test:\n",
    "                examples = processor.get_test_examples(data_dir)\n",
    "            else:\n",
    "                examples = processor.get_train_examples(data_dir)\n",
    "            logger.info(\"Training examples: %s\", len(examples))\n",
    "            # TODO clean up all this to leverage built-in features of tokenizers\n",
    "            self.features = convert_examples_to_features(\n",
    "                examples,\n",
    "                label_list,\n",
    "                max_seq_length,\n",
    "                tokenizer,\n",
    "                pad_on_left=bool(tokenizer.padding_side == \"left\"),\n",
    "                pad_token=tokenizer.pad_token_id,\n",
    "                pad_token_segment_id=tokenizer.pad_token_type_id,\n",
    "            )\n",
    "\n",
    "            def gen():\n",
    "                for (ex_index, ex) in tqdm.tqdm(enumerate(self.features), desc=\"convert examples to features\"):\n",
    "                    if ex_index % 10000 == 0:\n",
    "                        logger.info(\"Writing example %d of %d\" % (ex_index, len(examples)))\n",
    "\n",
    "                    yield (\n",
    "                        {\n",
    "                            \"example_id\": 0,\n",
    "                            \"input_ids\": ex.input_ids,\n",
    "                            \"attention_mask\": ex.attention_mask,\n",
    "                            \"token_type_ids\": ex.token_type_ids,\n",
    "                        },\n",
    "                        ex.label,\n",
    "                    )\n",
    "\n",
    "            self.dataset = tf.data.Dataset.from_generator(\n",
    "                gen,\n",
    "                (\n",
    "                    {\n",
    "                        \"example_id\": tf.int32,\n",
    "                        \"input_ids\": tf.int32,\n",
    "                        \"attention_mask\": tf.int32,\n",
    "                        \"token_type_ids\": tf.int32,\n",
    "                    },\n",
    "                    tf.int64,\n",
    "                ),\n",
    "                (\n",
    "                    {\n",
    "                        \"example_id\": tf.TensorShape([]),\n",
    "                        \"input_ids\": tf.TensorShape([None, None]),\n",
    "                        \"attention_mask\": tf.TensorShape([None, None]),\n",
    "                        \"token_type_ids\": tf.TensorShape([None, None]),\n",
    "                    },\n",
    "                    tf.TensorShape([]),\n",
    "                ),\n",
    "            )\n",
    "\n",
    "        def get_dataset(self):\n",
    "            return self.dataset\n",
    "\n",
    "        def __len__(self):\n",
    "            return len(self.features)\n",
    "\n",
    "        def __getitem__(self, i) -> InputFeatures:\n",
    "            return self.features[i]\n",
    "\n",
    "\n",
    "class DataProcessor:\n",
    "    \"\"\"Base class for data converters for multiple choice data sets.\"\"\"\n",
    "\n",
    "    def get_train_examples(self, data_dir):\n",
    "        \"\"\"Gets a collection of `InputExample`s for the train set.\"\"\"\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    def get_dev_examples(self, data_dir):\n",
    "        \"\"\"Gets a collection of `InputExample`s for the dev set.\"\"\"\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    def get_test_examples(self, data_dir):\n",
    "        \"\"\"Gets a collection of `InputExample`s for the test set.\"\"\"\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    def get_labels(self):\n",
    "        \"\"\"Gets the list of labels for this data set.\"\"\"\n",
    "        raise NotImplementedError()\n",
    "\n",
    "\n",
    "class RaceProcessor(DataProcessor):\n",
    "    \"\"\"Processor for the RACE data set.\"\"\"\n",
    "\n",
    "    def get_train_examples(self, data_dir):\n",
    "        \"\"\"See base class.\"\"\"\n",
    "        logger.info(\"LOOKING AT {} train\".format(data_dir))\n",
    "        high = os.path.join(data_dir, \"train/high\")\n",
    "        middle = os.path.join(data_dir, \"train/middle\")\n",
    "        high = self._read_txt(high)\n",
    "        middle = self._read_txt(middle)\n",
    "        return self._create_examples(high + middle, \"train\")\n",
    "\n",
    "    def get_dev_examples(self, data_dir):\n",
    "        \"\"\"See base class.\"\"\"\n",
    "        logger.info(\"LOOKING AT {} dev\".format(data_dir))\n",
    "        high = os.path.join(data_dir, \"dev/high\")\n",
    "        middle = os.path.join(data_dir, \"dev/middle\")\n",
    "        high = self._read_txt(high)\n",
    "        middle = self._read_txt(middle)\n",
    "        return self._create_examples(high + middle, \"dev\")\n",
    "\n",
    "    def get_test_examples(self, data_dir):\n",
    "        \"\"\"See base class.\"\"\"\n",
    "        logger.info(\"LOOKING AT {} test\".format(data_dir))\n",
    "        high = os.path.join(data_dir, \"test/high\")\n",
    "        middle = os.path.join(data_dir, \"test/middle\")\n",
    "        high = self._read_txt(high)\n",
    "        middle = self._read_txt(middle)\n",
    "        return self._create_examples(high + middle, \"test\")\n",
    "\n",
    "    def get_labels(self):\n",
    "        \"\"\"See base class.\"\"\"\n",
    "        return [\"0\", \"1\", \"2\", \"3\"]\n",
    "\n",
    "    def _read_txt(self, input_dir):\n",
    "        lines = []\n",
    "        files = glob.glob(input_dir + \"/*txt\")\n",
    "        for file in tqdm.tqdm(files, desc=\"read files\"):\n",
    "            with open(file, \"r\", encoding=\"utf-8\") as fin:\n",
    "                data_raw = json.load(fin)\n",
    "                data_raw[\"race_id\"] = file\n",
    "                lines.append(data_raw)\n",
    "        return lines\n",
    "\n",
    "    def _create_examples(self, lines, set_type):\n",
    "        \"\"\"Creates examples for the training and dev sets.\"\"\"\n",
    "        examples = []\n",
    "        for (_, data_raw) in enumerate(lines):\n",
    "            race_id = \"%s-%s\" % (set_type, data_raw[\"race_id\"])\n",
    "            article = data_raw[\"article\"]\n",
    "            for i in range(len(data_raw[\"answers\"])):\n",
    "                truth = str(ord(data_raw[\"answers\"][i]) - ord(\"A\"))\n",
    "                question = data_raw[\"questions\"][i]\n",
    "                options = data_raw[\"options\"][i]\n",
    "\n",
    "                examples.append(\n",
    "                    InputExample(\n",
    "                        example_id=race_id,\n",
    "                        question=question,\n",
    "                        contexts=[article, article, article, article],  # this is not efficient but convenient\n",
    "                        endings=[options[0], options[1], options[2], options[3]],\n",
    "                        label=truth,\n",
    "                    )\n",
    "                )\n",
    "        return examples\n",
    "\n",
    "\n",
    "class SynonymProcessor(DataProcessor):\n",
    "    \"\"\"Processor for the Synonym data set.\"\"\"\n",
    "\n",
    "    def get_train_examples(self, data_dir):\n",
    "        \"\"\"See base class.\"\"\"\n",
    "        logger.info(\"LOOKING AT {} train\".format(data_dir))\n",
    "        return self._create_examples(self._read_csv(os.path.join(data_dir, \"mctrain.csv\")), \"train\")\n",
    "\n",
    "    def get_dev_examples(self, data_dir):\n",
    "        \"\"\"See base class.\"\"\"\n",
    "        logger.info(\"LOOKING AT {} dev\".format(data_dir))\n",
    "        return self._create_examples(self._read_csv(os.path.join(data_dir, \"mchp.csv\")), \"dev\")\n",
    "\n",
    "    def get_test_examples(self, data_dir):\n",
    "        \"\"\"See base class.\"\"\"\n",
    "        logger.info(\"LOOKING AT {} dev\".format(data_dir))\n",
    "\n",
    "        return self._create_examples(self._read_csv(os.path.join(data_dir, \"mctest.csv\")), \"test\")\n",
    "\n",
    "    def get_labels(self):\n",
    "        \"\"\"See base class.\"\"\"\n",
    "        return [\"0\", \"1\", \"2\", \"3\", \"4\"]\n",
    "\n",
    "    def _read_csv(self, input_file):\n",
    "        with open(input_file, \"r\", encoding=\"utf-8\") as f:\n",
    "            return list(csv.reader(f))\n",
    "\n",
    "    def _create_examples(self, lines: List[List[str]], type: str):\n",
    "        \"\"\"Creates examples for the training and dev sets.\"\"\"\n",
    "\n",
    "        examples = [\n",
    "            InputExample(\n",
    "                example_id=line[0],\n",
    "                question=\"\",  # in the swag dataset, the\n",
    "                # common beginning of each\n",
    "                # choice is stored in \"sent2\".\n",
    "                contexts=[line[1], line[1], line[1], line[1], line[1]],\n",
    "                endings=[line[2], line[3], line[4], line[5], line[6]],\n",
    "                label=line[7],\n",
    "            )\n",
    "            for line in lines  # we skip the line with the column names\n",
    "        ]\n",
    "\n",
    "        return examples\n",
    "\n",
    "\n",
    "class SwagProcessor(DataProcessor):\n",
    "    \"\"\"Processor for the SWAG data set.\"\"\"\n",
    "\n",
    "    def get_train_examples(self, data_dir):\n",
    "        \"\"\"See base class.\"\"\"\n",
    "        logger.info(\"LOOKING AT {} train\".format(data_dir))\n",
    "        return self._create_examples(self._read_csv(os.path.join(data_dir, \"train.csv\")), \"train\")\n",
    "\n",
    "    def get_dev_examples(self, data_dir):\n",
    "        \"\"\"See base class.\"\"\"\n",
    "        logger.info(\"LOOKING AT {} dev\".format(data_dir))\n",
    "        return self._create_examples(self._read_csv(os.path.join(data_dir, \"val.csv\")), \"dev\")\n",
    "\n",
    "    def get_test_examples(self, data_dir):\n",
    "        \"\"\"See base class.\"\"\"\n",
    "        logger.info(\"LOOKING AT {} dev\".format(data_dir))\n",
    "        raise ValueError(\n",
    "            \"For swag testing, the input file does not contain a label column. It can not be tested in current code\"\n",
    "            \"setting!\"\n",
    "        )\n",
    "        return self._create_examples(self._read_csv(os.path.join(data_dir, \"test.csv\")), \"test\")\n",
    "\n",
    "    def get_labels(self):\n",
    "        \"\"\"See base class.\"\"\"\n",
    "        return [\"0\", \"1\", \"2\", \"3\"]\n",
    "\n",
    "    def _read_csv(self, input_file):\n",
    "        with open(input_file, \"r\", encoding=\"utf-8\") as f:\n",
    "            return list(csv.reader(f))\n",
    "\n",
    "    def _create_examples(self, lines: List[List[str]], type: str):\n",
    "        \"\"\"Creates examples for the training and dev sets.\"\"\"\n",
    "        if type == \"train\" and lines[0][-1] != \"label\":\n",
    "            raise ValueError(\"For training, the input file must contain a label column.\")\n",
    "\n",
    "        examples = [\n",
    "            InputExample(\n",
    "                example_id=line[2],\n",
    "                question=line[5],  # in the swag dataset, the\n",
    "                # common beginning of each\n",
    "                # choice is stored in \"sent2\".\n",
    "                contexts=[line[4], line[4], line[4], line[4]],\n",
    "                endings=[line[7], line[8], line[9], line[10]],\n",
    "                label=line[11],\n",
    "            )\n",
    "            for line in lines[1:]  # we skip the line with the column names\n",
    "        ]\n",
    "\n",
    "        return examples\n",
    "\n",
    "\n",
    "class ArcProcessor(DataProcessor):\n",
    "    \"\"\"Processor for the ARC data set (request from allennlp).\"\"\"\n",
    "\n",
    "    def get_train_examples(self, data_dir):\n",
    "        \"\"\"See base class.\"\"\"\n",
    "        logger.info(\"LOOKING AT {} train\".format(data_dir))\n",
    "        return self._create_examples(self._read_json(os.path.join(data_dir, \"train.jsonl\")), \"train\")\n",
    "\n",
    "    def get_dev_examples(self, data_dir):\n",
    "        \"\"\"See base class.\"\"\"\n",
    "        logger.info(\"LOOKING AT {} dev\".format(data_dir))\n",
    "        return self._create_examples(self._read_json(os.path.join(data_dir, \"dev.jsonl\")), \"dev\")\n",
    "\n",
    "    def get_test_examples(self, data_dir):\n",
    "        logger.info(\"LOOKING AT {} test\".format(data_dir))\n",
    "        return self._create_examples(self._read_json(os.path.join(data_dir, \"test.jsonl\")), \"test\")\n",
    "\n",
    "    def get_labels(self):\n",
    "        \"\"\"See base class.\"\"\"\n",
    "        return [\"0\", \"1\", \"2\", \"3\"]\n",
    "\n",
    "    def _read_json(self, input_file):\n",
    "        with open(input_file, \"r\", encoding=\"utf-8\") as fin:\n",
    "            lines = fin.readlines()\n",
    "            return lines\n",
    "\n",
    "    def _create_examples(self, lines, type):\n",
    "        \"\"\"Creates examples for the training and dev sets.\"\"\"\n",
    "\n",
    "        # There are two types of labels. They should be normalized\n",
    "        def normalize(truth):\n",
    "            if truth in \"ABCD\":\n",
    "                return ord(truth) - ord(\"A\")\n",
    "            elif truth in \"1234\":\n",
    "                return int(truth) - 1\n",
    "            else:\n",
    "                logger.info(\"truth ERROR! %s\", str(truth))\n",
    "                return None\n",
    "\n",
    "        examples = []\n",
    "        three_choice = 0\n",
    "        four_choice = 0\n",
    "        five_choice = 0\n",
    "        other_choices = 0\n",
    "        # we deleted example which has more than or less than four choices\n",
    "        for line in tqdm.tqdm(lines, desc=\"read arc data\"):\n",
    "            data_raw = json.loads(line.strip(\"\\n\"))\n",
    "            if len(data_raw[\"question\"][\"choices\"]) == 3:\n",
    "                three_choice += 1\n",
    "                continue\n",
    "            elif len(data_raw[\"question\"][\"choices\"]) == 5:\n",
    "                five_choice += 1\n",
    "                continue\n",
    "            elif len(data_raw[\"question\"][\"choices\"]) != 4:\n",
    "                other_choices += 1\n",
    "                continue\n",
    "            four_choice += 1\n",
    "            truth = str(normalize(data_raw[\"answerKey\"]))\n",
    "            assert truth != \"None\"\n",
    "            question_choices = data_raw[\"question\"]\n",
    "            question = question_choices[\"stem\"]\n",
    "            id = data_raw[\"id\"]\n",
    "            options = question_choices[\"choices\"]\n",
    "            if len(options) == 4:\n",
    "                examples.append(\n",
    "                    InputExample(\n",
    "                        example_id=id,\n",
    "                        question=question,\n",
    "                        contexts=[\n",
    "                            options[0][\"para\"].replace(\"_\", \"\"),\n",
    "                            options[1][\"para\"].replace(\"_\", \"\"),\n",
    "                            options[2][\"para\"].replace(\"_\", \"\"),\n",
    "                            options[3][\"para\"].replace(\"_\", \"\"),\n",
    "                        ],\n",
    "                        endings=[options[0][\"text\"], options[1][\"text\"], options[2][\"text\"], options[3][\"text\"]],\n",
    "                        label=truth,\n",
    "                    )\n",
    "                )\n",
    "\n",
    "        if type == \"train\":\n",
    "            assert len(examples) > 1\n",
    "            assert examples[0].label is not None\n",
    "        logger.info(\"len examples: %s}\", str(len(examples)))\n",
    "        logger.info(\"Three choices: %s\", str(three_choice))\n",
    "        logger.info(\"Five choices: %s\", str(five_choice))\n",
    "        logger.info(\"Other choices: %s\", str(other_choices))\n",
    "        logger.info(\"four choices: %s\", str(four_choice))\n",
    "\n",
    "        return examples\n",
    "\n",
    "\n",
    "def convert_examples_to_features(\n",
    "    examples: List[InputExample],\n",
    "    label_list: List[str],\n",
    "    max_length: int,\n",
    "    tokenizer: PreTrainedTokenizer,\n",
    "    pad_token_segment_id=0,\n",
    "    pad_on_left=False,\n",
    "    pad_token=0,\n",
    "    mask_padding_with_zero=True,\n",
    ") -> List[InputFeatures]:\n",
    "    \"\"\"\n",
    "    Loads a data file into a list of `InputFeatures`\n",
    "    \"\"\"\n",
    "\n",
    "    label_map = {label: i for i, label in enumerate(label_list)}\n",
    "\n",
    "    features = []\n",
    "    for (ex_index, example) in tqdm.tqdm(enumerate(examples), desc=\"convert examples to features\"):\n",
    "        if ex_index % 10000 == 0:\n",
    "            logger.info(\"Writing example %d of %d\" % (ex_index, len(examples)))\n",
    "        choices_inputs = []\n",
    "        for ending_idx, (context, ending) in enumerate(zip(example.contexts, example.endings)):\n",
    "            text_a = context\n",
    "            if example.question.find(\"_\") != -1:\n",
    "                # this is for cloze question\n",
    "                text_b = example.question.replace(\"_\", ending)\n",
    "            else:\n",
    "                text_b = example.question + \" \" + ending\n",
    "\n",
    "            inputs = tokenizer.encode_plus(\n",
    "                text_a,\n",
    "                text_b,\n",
    "                add_special_tokens=True,\n",
    "                max_length=max_length,\n",
    "                pad_to_max_length=True,\n",
    "                return_overflowing_tokens=True,\n",
    "            )\n",
    "            if \"num_truncated_tokens\" in inputs and inputs[\"num_truncated_tokens\"] > 0:\n",
    "                logger.info(\n",
    "                    \"Attention! you are cropping tokens (swag task is ok). \"\n",
    "                    \"If you are training ARC and RACE and you are poping question + options,\"\n",
    "                    \"you need to try to use a bigger max seq length!\"\n",
    "                )\n",
    "\n",
    "            choices_inputs.append(inputs)\n",
    "\n",
    "        label = label_map[example.label]\n",
    "\n",
    "        input_ids = [x[\"input_ids\"] for x in choices_inputs]\n",
    "        attention_mask = (\n",
    "            [x[\"attention_mask\"] for x in choices_inputs] if \"attention_mask\" in choices_inputs[0] else None\n",
    "        )\n",
    "        token_type_ids = (\n",
    "            [x[\"token_type_ids\"] for x in choices_inputs] if \"token_type_ids\" in choices_inputs[0] else None\n",
    "        )\n",
    "\n",
    "        features.append(\n",
    "            InputFeatures(\n",
    "                example_id=example.example_id,\n",
    "                input_ids=input_ids,\n",
    "                attention_mask=attention_mask,\n",
    "                token_type_ids=token_type_ids,\n",
    "                label=label,\n",
    "            )\n",
    "        )\n",
    "\n",
    "    for f in features[:2]:\n",
    "        logger.info(\"*** Example ***\")\n",
    "        logger.info(\"feature: %s\" % f)\n",
    "\n",
    "    return features\n",
    "\n",
    "\n",
    "processors = {\"race\": RaceProcessor, \"swag\": SwagProcessor, \"arc\": ArcProcessor, \"syn\": SynonymProcessor}\n",
    "MULTIPLE_CHOICE_TASKS_NUM_LABELS = {\"race\", 4, \"swag\", 4, \"arc\", 4, \"syn\", 5}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import os\n",
    "from dataclasses import dataclass, field\n",
    "from typing import Dict, Optional\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from transformers import (\n",
    "    AutoConfig,\n",
    "    AutoModelForMultipleChoice,\n",
    "    AutoTokenizer,\n",
    "    EvalPrediction,\n",
    "    HfArgumentParser,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    "    set_seed,\n",
    ")\n",
    "# from utils_multiple_choice import MultipleChoiceDataset, Split, processors\n",
    "\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "\n",
    "def simple_accuracy(preds, labels):\n",
    "    return (preds == labels).mean()\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class ModelArguments:\n",
    "    \"\"\"\n",
    "    Arguments pertaining to which model/config/tokenizer we are going to fine-tune from.\n",
    "    \"\"\"\n",
    "\n",
    "    model_name_or_path: str = field(\n",
    "        metadata={\"help\": \"Path to pretrained model or model identifier from huggingface.co/models\"}\n",
    "    )\n",
    "    config_name: Optional[str] = field(\n",
    "        default=None, metadata={\"help\": \"Pretrained config name or path if not the same as model_name\"}\n",
    "    )\n",
    "    tokenizer_name: Optional[str] = field(\n",
    "        default=None, metadata={\"help\": \"Pretrained tokenizer name or path if not the same as model_name\"}\n",
    "    )\n",
    "    cache_dir: Optional[str] = field(\n",
    "        default=None, metadata={\"help\": \"Where do you want to store the pretrained models downloaded from s3\"}\n",
    "    )\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class DataTrainingArguments:\n",
    "    \"\"\"\n",
    "    Arguments pertaining to what data we are going to input our model for training and eval.\n",
    "    \"\"\"\n",
    "\n",
    "    task_name: str = field(metadata={\"help\": \"The name of the task to train on: \" + \", \".join(processors.keys())})\n",
    "    data_dir: str = field(metadata={\"help\": \"Should contain the data files for the task.\"})\n",
    "    max_seq_length: int = field(\n",
    "        default=128,\n",
    "        metadata={\n",
    "            \"help\": \"The maximum total input sequence length after tokenization. Sequences longer \"\n",
    "            \"than this will be truncated, sequences shorter will be padded.\"\n",
    "        },\n",
    "    )\n",
    "    overwrite_cache: bool = field(\n",
    "        default=False, metadata={\"help\": \"Overwrite the cached training and evaluation sets\"}\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "args=['--model_name_or_path', 'bert-base-uncased',\n",
    "      '--task_name', 'swag',\n",
    "      '--data_dir', './swag',\n",
    "      '--do_train',\n",
    "      '--do_eval',\n",
    "      '--max_seq_length', '256',\n",
    "       '--per_device_train_batch_size', '32',\n",
    "      '--learning_rate', '2e-5',\n",
    "       '--num_train_epochs', '5.0',\n",
    "       '--output_dir', './',\n",
    "       '--overwrite_output_dir']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "parser = HfArgumentParser((ModelArguments, DataTrainingArguments, TrainingArguments))\n",
    "model_args, data_args, training_args = parser.parse_args_into_dataclasses(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "if (\n",
    "    os.path.exists(training_args.output_dir)\n",
    "    and os.listdir(training_args.output_dir)\n",
    "    and training_args.do_train\n",
    "    and not training_args.overwrite_output_dir\n",
    "):\n",
    "    raise ValueError(\n",
    "        f\"Output directory ({training_args.output_dir}) already exists and is not empty. Use --overwrite_output_dir to overcome.\"\n",
    "    )\n",
    "\n",
    "# Setup logging\n",
    "logging.basicConfig(\n",
    "    format=\"%(asctime)s - %(levelname)s - %(name)s -   %(message)s\",\n",
    "    datefmt=\"%m/%d/%Y %H:%M:%S\",\n",
    "    level=logging.INFO if training_args.local_rank in [-1, 0] else logging.WARN,\n",
    ")\n",
    "logger.warning(\n",
    "    \"Process rank: %s, device: %s, n_gpu: %s, distributed training: %s, 16-bits training: %s\",\n",
    "    training_args.local_rank,\n",
    "    training_args.device,\n",
    "    training_args.n_gpu,\n",
    "    bool(training_args.local_rank != -1),\n",
    "    training_args.fp16,\n",
    ")\n",
    "logger.info(\"Training/evaluation parameters %s\", training_args)\n",
    "\n",
    "# Set seed\n",
    "set_seed(training_args.seed)\n",
    "\n",
    "try:\n",
    "    processor = processors[data_args.task_name]()\n",
    "    label_list = processor.get_labels()\n",
    "    num_labels = len(label_list)\n",
    "except KeyError:\n",
    "    raise ValueError(\"Task not found: %s\" % (data_args.task_name))\n",
    "\n",
    "# Load pretrained model and tokenizer\n",
    "# Distributed training:\n",
    "# The .from_pretrained methods guarantee that only one local process can concurrently\n",
    "# download model & vocab.\n",
    "\n",
    "config = AutoConfig.from_pretrained(\n",
    "    model_args.config_name if model_args.config_name else model_args.model_name_or_path,\n",
    "    num_labels=num_labels,\n",
    "    finetuning_task=data_args.task_name,\n",
    "    cache_dir=model_args.cache_dir,\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    model_args.tokenizer_name if model_args.tokenizer_name else model_args.model_name_or_path,\n",
    "    cache_dir=model_args.cache_dir,\n",
    ")\n",
    "\n",
    "model = AutoModelForMultipleChoice.from_pretrained(\n",
    "    model_args.model_name_or_path,\n",
    "    from_tf=bool(\".ckpt\" in model_args.model_name_or_path),\n",
    "    config=config,\n",
    "    cache_dir=model_args.cache_dir,\n",
    ")\n",
    "\n",
    "# Get datasets\n",
    "train_dataset = (\n",
    "    MultipleChoiceDataset(\n",
    "        data_dir=data_args.data_dir,\n",
    "        tokenizer=tokenizer,\n",
    "        task=data_args.task_name,\n",
    "        max_seq_length=data_args.max_seq_length,\n",
    "        overwrite_cache=data_args.overwrite_cache,\n",
    "        mode=Split.train,\n",
    "    )\n",
    "    if training_args.do_train\n",
    "    else None\n",
    ")\n",
    "eval_dataset = (\n",
    "    MultipleChoiceDataset(\n",
    "        data_dir=data_args.data_dir,\n",
    "        tokenizer=tokenizer,\n",
    "        task=data_args.task_name,\n",
    "        max_seq_length=data_args.max_seq_length,\n",
    "        overwrite_cache=data_args.overwrite_cache,\n",
    "        mode=Split.dev,\n",
    "    )\n",
    "    if training_args.do_eval\n",
    "    else None\n",
    ")\n",
    "\n",
    "def compute_metrics(p: EvalPrediction) -> Dict:\n",
    "    preds = np.argmax(p.predictions, axis=1)\n",
    "    return {\"acc\": simple_accuracy(preds, p.label_ids)}\n",
    "\n",
    "# Initialize our Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=eval_dataset,\n",
    "    compute_metrics=compute_metrics,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "eval_dataset.features[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dl = DataLoader(\n",
    "            train_dataset,\n",
    "            collate_fn=default_data_collator,\n",
    "            batch_size=8,\n",
    "            drop_last=False,\n",
    "            num_workers=2,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = iter(train_dl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "b = next(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pghjkk = model(**b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids = b['input_ids']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_choices = input_ids.shape[1] if input_ids is not None else inputs_embeds.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_choices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "attention_mask.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids = input_ids.view(-1, input_ids.size(-1)) if input_ids is not None else None\n",
    "attention_mask = b['attention_mask'].view(-1, b['attention_mask'].size(-1))\n",
    "token_type_ids = b['token_type_ids'].view(-1, b['token_type_ids'].size(-1))\n",
    "position_ids = None#b['position_ids'].view(-1, b['position_ids'].size(-1)) if b['position_ids'] is not None else None\n",
    "\n",
    "inputs_embeds = None# (\n",
    "#     b['inputs_embeds'].view(-1, b['inputs_embeds'].size(-2), b['inputs_embeds'].size(-1))\n",
    "#     if b['inputs_embeds'] is not None\n",
    "#     else None\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from_bert_only = model.bert(\n",
    "            input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            token_type_ids=token_type_ids,\n",
    "            position_ids=position_ids,\n",
    "            head_mask=None,\n",
    "            inputs_embeds=inputs_embeds,\n",
    "            output_attentions=None,\n",
    "            output_hidden_states=None,\n",
    "            return_dict=None,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b['position_ids']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b['input_i'] is not None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from_bert_only[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids = input_ids.view(-1, input_ids.size(-1)) if input_ids is not None else None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from_bert_only[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "len(from_bert_only)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dropout = nn.Dropout(0.1)\n",
    "classifier = nn.Linear(768, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from_bert_only[2][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from_bert_only[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pooled_output = from_bert_only[1]\n",
    "\n",
    "pooled_output = dropout(pooled_output)\n",
    "logits = classifier(pooled_output)\n",
    "reshaped_logits = logits.view(-1, num_choices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reshaped_logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from_bert_only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b['input_ids'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pghjkk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training\n",
    "if training_args.do_train:\n",
    "    trainer.train(\n",
    "        model_path=model_args.model_name_or_path if os.path.isdir(model_args.model_name_or_path) else None\n",
    "    )\n",
    "    trainer.save_model()\n",
    "    # For convenience, we also re-save the tokenizer to the same directory,\n",
    "    # so that you can share your model easily on huggingface.co/models =)\n",
    "    if trainer.is_world_master():\n",
    "        tokenizer.save_pretrained(training_args.output_dir)\n",
    "\n",
    "# Evaluation\n",
    "results = {}\n",
    "if training_args.do_eval:\n",
    "    logger.info(\"*** Evaluate ***\")\n",
    "\n",
    "    result = trainer.evaluate()\n",
    "\n",
    "    output_eval_file = os.path.join(training_args.output_dir, \"eval_results.txt\")\n",
    "    if trainer.is_world_master():\n",
    "        with open(output_eval_file, \"w\") as writer:\n",
    "            logger.info(\"***** Eval results *****\")\n",
    "            for key, value in result.items():\n",
    "                logger.info(\"  %s = %s\", key, value)\n",
    "                writer.write(\"%s = %s\\n\" % (key, value))\n",
    "\n",
    "            results.update(result)\n",
    "\n",
    "return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = pd.read_csv('./swag/train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_dataset.features[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset.features[0].input_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.convert_ids_to_tokens(102)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.convert_tokens_to_ids('someone')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "t[t['fold-ind'] == 3416].iloc[0]['sent1']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t.iloc[[102]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t.iloc[[102]]['startphrase'].item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t.iloc[[100]]['ending0'].item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t.iloc[[100]]['ending1'].item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "**b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t.iloc[[100]]['ending2'].item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t.iloc[[100]]['ending3'].item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "eecs595",
   "language": "python",
   "name": "eecs595"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
