{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import os\n",
    "import random\n",
    "import sys\n",
    "from dataclasses import dataclass, field\n",
    "from typing import Optional\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
    "\n",
    "import numpy as np\n",
    "from datasets import load_dataset, load_metric\n",
    "\n",
    "import transformers\n",
    "from transformers import (\n",
    "    AutoConfig,\n",
    "    AutoModelForSequenceClassification,\n",
    "    AutoTokenizer,\n",
    "    EvalPrediction,\n",
    "    HfArgumentParser,\n",
    "    PretrainedConfig,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    "    default_data_collator,\n",
    "    set_seed,\n",
    ")\n",
    "from transformers.trainer_utils import is_main_process\n",
    "\n",
    "\n",
    "task_to_keys = {\n",
    "    \"cola\": (\"sentence\", None),\n",
    "    \"mnli\": (\"premise\", \"hypothesis\"),\n",
    "    \"mrpc\": (\"sentence1\", \"sentence2\"),\n",
    "    \"qnli\": (\"question\", \"sentence\"),\n",
    "    \"qqp\": (\"question1\", \"question2\"),\n",
    "    \"rte\": (\"sentence1\", \"sentence2\"),\n",
    "    \"sst2\": (\"sentence\", None),\n",
    "    \"stsb\": (\"sentence1\", \"sentence2\"),\n",
    "    \"wnli\": (\"sentence1\", \"sentence2\"),\n",
    "}\n",
    "\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class DataTrainingArguments:\n",
    "    \"\"\"\n",
    "    Arguments pertaining to what data we are going to input our model for training and eval.\n",
    "    Using `HfArgumentParser` we can turn this class\n",
    "    into argparse arguments to be able to specify them on\n",
    "    the command line.\n",
    "    \"\"\"\n",
    "\n",
    "    task_name: Optional[str] = field(\n",
    "        default=None,\n",
    "        metadata={\"help\": \"The name of the task to train on: \" + \", \".join(task_to_keys.keys())},\n",
    "    )\n",
    "    max_seq_length: int = field(\n",
    "        default=128,\n",
    "        metadata={\n",
    "            \"help\": \"The maximum total input sequence length after tokenization. Sequences longer \"\n",
    "            \"than this will be truncated, sequences shorter will be padded.\"\n",
    "        },\n",
    "    )\n",
    "    overwrite_cache: bool = field(\n",
    "        default=False, metadata={\"help\": \"Overwrite the cached preprocessed datasets or not.\"}\n",
    "    )\n",
    "    pad_to_max_length: bool = field(\n",
    "        default=True,\n",
    "        metadata={\n",
    "            \"help\": \"Whether to pad all samples to `max_seq_length`. \"\n",
    "            \"If False, will pad the samples dynamically when batching to the maximum length in the batch.\"\n",
    "        },\n",
    "    )\n",
    "    train_file: Optional[str] = field(\n",
    "        default=None, metadata={\"help\": \"A csv or a json file containing the training data.\"}\n",
    "    )\n",
    "    validation_file: Optional[str] = field(\n",
    "        default=None, metadata={\"help\": \"A csv or a json file containing the validation data.\"}\n",
    "    )\n",
    "\n",
    "    def __post_init__(self):\n",
    "        if self.task_name is not None:\n",
    "            self.task_name = self.task_name.lower()\n",
    "            if self.task_name not in task_to_keys.keys():\n",
    "                raise ValueError(\"Unknown task, you should pick one in \" + \",\".join(task_to_keys.keys()))\n",
    "        elif self.train_file is None or self.validation_file is None:\n",
    "            raise ValueError(\"Need either a GLUE task or a training/validation file.\")\n",
    "        else:\n",
    "            extension = self.train_file.split(\".\")[-1]\n",
    "            assert extension in [\"csv\", \"json\"], \"`train_file` should be a csv or a json file.\"\n",
    "            extension = self.validation_file.split(\".\")[-1]\n",
    "            assert extension in [\"csv\", \"json\"], \"`validation_file` should be a csv or a json file.\"\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class ModelArguments:\n",
    "    \"\"\"\n",
    "    Arguments pertaining to which model/config/tokenizer we are going to fine-tune from.\n",
    "    \"\"\"\n",
    "\n",
    "    model_name_or_path: str = field(\n",
    "        metadata={\"help\": \"Path to pretrained model or model identifier from huggingface.co/models\"}\n",
    "    )\n",
    "    config_name: Optional[str] = field(\n",
    "        default=None, metadata={\"help\": \"Pretrained config name or path if not the same as model_name\"}\n",
    "    )\n",
    "    tokenizer_name: Optional[str] = field(\n",
    "        default=None, metadata={\"help\": \"Pretrained tokenizer name or path if not the same as model_name\"}\n",
    "    )\n",
    "    cache_dir: Optional[str] = field(\n",
    "        default=None, metadata={\"help\": \"Where do you want to store the pretrained models downloaded from s3\"}\n",
    "    )\n",
    "    use_fast_tokenizer: bool = field(\n",
    "        default=True,\n",
    "        metadata={\"help\": \"Whether to use one of the fast tokenizer (backed by the tokenizers library) or not.\"},\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "args=['--model_name_or_path', 'bert-base-uncased',\n",
    "#       '--task_name', 'rte',\n",
    "      '--train_file', 'eat_train.csv',\n",
    "      '--validation_file', 'eat_test.csv',\n",
    "      '--do_train',\n",
    "      '--do_eval',\n",
    "      '--max_seq_length', '256',\n",
    "       '--per_device_train_batch_size', '32',\n",
    "      '--learning_rate', '2e-5',\n",
    "       '--num_train_epochs', '5.0',\n",
    "       '--output_dir', './',\n",
    "       '--overwrite_output_dir']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DataTrainingArguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_args.pad_to_max_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TrainingArguments(output_dir='./', overwrite_output_dir=True, do_train=True, do_eval=True, do_predict=False, evaluate_during_training=False, evaluation_strategy=<EvaluationStrategy.NO: 'no'>, prediction_loss_only=False, per_device_train_batch_size=32, per_device_eval_batch_size=8, per_gpu_train_batch_size=None, per_gpu_eval_batch_size=None, gradient_accumulation_steps=1, eval_accumulation_steps=None, learning_rate=2e-05, weight_decay=0.0, adam_beta1=0.9, adam_beta2=0.999, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=5.0, max_steps=-1, warmup_steps=0, logging_dir='runs/Nov08_18-45-01_dcex-apsc01', logging_first_step=False, logging_steps=500, save_steps=500, save_total_limit=None, no_cuda=False, seed=42, fp16=False, fp16_opt_level='O1', local_rank=-1, tpu_num_cores=None, tpu_metrics_debug=False, debug=False, dataloader_drop_last=False, eval_steps=500, dataloader_num_workers=0, past_index=-1, run_name='./', disable_tqdm=False, remove_unused_columns=True, label_names=None, load_best_model_at_end=False, metric_for_best_model=None, greater_is_better=None)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "parser = HfArgumentParser((ModelArguments, DataTrainingArguments, TrainingArguments))\n",
    "\n",
    "model_args, data_args, training_args = parser.parse_args_into_dataclasses(args)\n",
    "\n",
    "if (\n",
    "    os.path.exists(training_args.output_dir)\n",
    "    and os.listdir(training_args.output_dir)\n",
    "    and training_args.do_train\n",
    "    and not training_args.overwrite_output_dir\n",
    "):\n",
    "    raise ValueError(\n",
    "        f\"Output directory ({training_args.output_dir}) already exists and is not empty. \"\n",
    "        \"Use --overwrite_output_dir to overcome.\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup logging\n",
    "logging.basicConfig(\n",
    "    format=\"%(asctime)s - %(levelname)s - %(name)s -   %(message)s\",\n",
    "    datefmt=\"%m/%d/%Y %H:%M:%S\",\n",
    "    level=logging.INFO if is_main_process(training_args.local_rank) else logging.WARN,\n",
    ")\n",
    "\n",
    "# Log on each process the small summary:\n",
    "logger.warning(\n",
    "    f\"Process rank: {training_args.local_rank}, device: {training_args.device}, n_gpu: {training_args.n_gpu}\"\n",
    "    + f\"distributed training: {bool(training_args.local_rank != -1)}, 16-bits training: {training_args.fp16}\"\n",
    ")\n",
    "# Set the verbosity to info of the Transformers logger (on main process only):\n",
    "if is_main_process(training_args.local_rank):\n",
    "    transformers.utils.logging.set_verbosity_info()\n",
    "logger.info(f\"Training/evaluation parameters {training_args}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set seed before initializing model.\n",
    "set_seed(training_args.seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Get the datasets: you can either provide your own CSV/JSON training and evaluation files (see below)\n",
    "# or specify a GLUE benchmark task (the dataset will be downloaded automatically from the datasets Hub).\n",
    "\n",
    "# For CSV/JSON files, this script will use as labels the column called 'label' and as pair of sentences the\n",
    "# sentences in columns called 'sentence1' and 'sentence2' if such column exists or the first two columns not named\n",
    "# label if at least two columns are provided.\n",
    "#\n",
    "# If the CSVs/JSONs contain only one non-label column, the script does single sentence classification on this\n",
    "# single column. You can easily tweak this behavior (see below)\n",
    "#\n",
    "# In distributed training, the load_dataset function guarantee that only one local process can concurrently\n",
    "# download the dataset.\n",
    "if data_args.task_name is not None:\n",
    "    # Downloading and loading a dataset from the hub.\n",
    "    datasets = load_dataset(\"glue\", data_args.task_name)\n",
    "elif data_args.train_file.endswith(\".csv\"):\n",
    "    # Loading a dataset from local csv files\n",
    "    datasets = load_dataset(\n",
    "        \"csv\", data_files={\"train\": data_args.train_file, \"validation\": data_args.validation_file}\n",
    "    )\n",
    "else:\n",
    "    # Loading a dataset from local json files\n",
    "    datasets = load_dataset(\n",
    "        \"json\", data_files={\"train\": data_args.train_file, \"validation\": data_args.validation_file}\n",
    "    )\n",
    "# See more about loading any type of standard or custom dataset at\n",
    "# https://huggingface.co/docs/datasets/loading_datasets.html.\n",
    "\n",
    "# Labels\n",
    "if data_args.task_name is not None:\n",
    "    is_regression = data_args.task_name == \"stsb\"\n",
    "    if not is_regression:\n",
    "        label_list = datasets[\"train\"].features[\"label\"].names\n",
    "        num_labels = len(label_list)\n",
    "    else:\n",
    "        num_labels = 1\n",
    "else:\n",
    "    # Trying to have good defaults here, don't hesitate to tweak to your needs.\n",
    "    is_regression = datasets[\"train\"].features[\"label\"].dtype in [\"float32\", \"float64\"]\n",
    "    if is_regression:\n",
    "        num_labels = 1\n",
    "    else:\n",
    "        # A useful fast method:\n",
    "        # https://huggingface.co/docs/datasets/package_reference/main_classes.html#datasets.Dataset.unique\n",
    "        label_list = datasets[\"train\"].unique(\"label\")\n",
    "        label_list.sort()  # Let's sort it for determinism\n",
    "        num_labels = len(label_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model and tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Load pretrained model and tokenizer\n",
    "#\n",
    "# In distributed training, the .from_pretrained methods guarantee that only one local process can concurrently\n",
    "# download model & vocab.\n",
    "config = AutoConfig.from_pretrained(\n",
    "    model_args.config_name if model_args.config_name else model_args.model_name_or_path,\n",
    "    num_labels=num_labels,\n",
    "    finetuning_task=data_args.task_name,\n",
    "    cache_dir=model_args.cache_dir,\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    model_args.tokenizer_name if model_args.tokenizer_name else model_args.model_name_or_path,\n",
    "    cache_dir=model_args.cache_dir,\n",
    "    use_fast=model_args.use_fast_tokenizer,\n",
    ")\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    model_args.model_name_or_path,\n",
    "    from_tf=bool(\".ckpt\" in model_args.model_name_or_path),\n",
    "    config=config,\n",
    "    cache_dir=model_args.cache_dir,\n",
    ")\n",
    "\n",
    "# Preprocessing the datasets\n",
    "if data_args.task_name is not None:\n",
    "    sentence1_key, sentence2_key = task_to_keys[data_args.task_name]\n",
    "else:\n",
    "    # Again, we try to have some nice defaults but don't hesitate to tweak to your use case.\n",
    "    non_label_column_names = [name for name in datasets[\"train\"].column_names if name != \"label\"]\n",
    "    if \"sentence1\" in non_label_column_names and \"sentence2\" in non_label_column_names:\n",
    "        sentence1_key, sentence2_key = \"sentence1\", \"sentence2\"\n",
    "    else:\n",
    "        if len(non_label_column_names) >= 2:\n",
    "            sentence1_key, sentence2_key = non_label_column_names[:2]\n",
    "        else:\n",
    "            sentence1_key, sentence2_key = non_label_column_names[0], None\n",
    "\n",
    "# Padding strategy\n",
    "if data_args.pad_to_max_length:\n",
    "    padding = \"max_length\"\n",
    "    max_length = data_args.max_seq_length\n",
    "else:\n",
    "    # We will pad later, dynamically at batch creation, to the max sequence length in each batch\n",
    "    padding = False\n",
    "    max_length = None\n",
    "\n",
    "# Some models have set the order of the labels to use, so let's make sure we do use it.\n",
    "label_to_id = None\n",
    "if (\n",
    "    model.config.label2id != PretrainedConfig(num_labels=num_labels).label2id\n",
    "    and data_args.task_name is not None\n",
    "    and is_regression\n",
    "):\n",
    "    # Some have all caps in their config, some don't.\n",
    "    label_name_to_id = {k.lower(): v for k, v in model.config.label2id.items()}\n",
    "    if list(sorted(label_name_to_id.keys())) == list(sorted(label_list)):\n",
    "        label_to_id = {i: label_name_to_id[label_list[i]] for i in range(num_labels)}\n",
    "    else:\n",
    "        logger.warn(\n",
    "            \"Your model seems to have been trained with labels, but they don't match the dataset: \",\n",
    "            f\"model labels: {list(sorted(label_name_to_id.keys()))}, dataset labels: {list(sorted(label_list))}.\"\n",
    "            \"\\nIgnoring the model labels as a result.\",\n",
    "        )\n",
    "elif data_args.task_name is None:\n",
    "    label_to_id = {v: i for i, v in enumerate(label_list)}\n",
    "\n",
    "def preprocess_function(examples):\n",
    "    # Tokenize the texts\n",
    "    args = (\n",
    "        (examples[sentence1_key],) if sentence2_key is None else (examples[sentence1_key], examples[sentence2_key])\n",
    "    )\n",
    "    result = tokenizer(*args, padding=padding, max_length=max_length, truncation=True)\n",
    "\n",
    "    # Map labels to IDs (not necessary for GLUE tasks)\n",
    "    if label_to_id is not None and \"label\" in examples:\n",
    "        result[\"label\"] = [label_to_id[l] for l in examples[\"label\"]]\n",
    "    return result\n",
    "\n",
    "datasets = datasets.map(preprocess_function, batched=True, load_from_cache_file=not data_args.overwrite_cache)\n",
    "\n",
    "train_dataset = datasets[\"train\"]\n",
    "eval_dataset = datasets[\"validation_matched\" if data_args.task_name == \"mnli\" else \"validation\"]\n",
    "if data_args.task_name is not None:\n",
    "    test_dataset = datasets[\"test_matched\" if data_args.task_name == \"mnli\" else \"test\"]\n",
    "\n",
    "# Log a few random samples from the training set:\n",
    "for index in random.sample(range(len(train_dataset)), 3):\n",
    "    logger.info(f\"Sample {index} of the training set: {train_dataset[index]}.\")\n",
    "\n",
    "# Get the metric function\n",
    "if data_args.task_name is not None:\n",
    "    metric = load_metric(\"glue\", data_args.task_name)\n",
    "# TODO: When datasets metrics include regular accuracy, make an else here and remove special branch from\n",
    "# compute_metrics\n",
    "\n",
    "# You can define your custom compute_metrics function. It takes an `EvalPrediction` object (a namedtuple with a\n",
    "# predictions and label_ids field) and has to return a dictionary string to float.\n",
    "def compute_metrics(p: EvalPrediction):\n",
    "    preds = p.predictions[0] if isinstance(p.predictions, tuple) else p.predictions\n",
    "    preds = np.squeeze(preds) if is_regression else np.argmax(preds, axis=1)\n",
    "    if data_args.task_name is not None:\n",
    "        result = metric.compute(predictions=preds, references=p.label_ids)\n",
    "        if len(result) > 1:\n",
    "            result[\"combined_score\"] = np.mean(list(result.values())).item()\n",
    "        return result\n",
    "    elif is_regression:\n",
    "        return {\"mse\": ((preds - p.label_ids) ** 2).mean().item()}\n",
    "    else:\n",
    "        return {\"accuracy\": (preds == p.label_ids).astype(np.float32).mean().item()}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Make sure it's a correct dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import datasets\n",
    "# import random\n",
    "# import pandas as pd\n",
    "# from IPython.display import display, HTML\n",
    "\n",
    "# def show_random_elements(dataset, num_examples=10):\n",
    "#     assert num_examples <= len(dataset), \"Can't pick more elements than there are in the dataset.\"\n",
    "#     picks = []\n",
    "#     for _ in range(num_examples):\n",
    "#         pick = random.randint(0, len(dataset)-1)\n",
    "#         while pick in picks:\n",
    "#             pick = random.randint(0, len(dataset)-1)\n",
    "#         picks.append(pick)\n",
    "    \n",
    "#     df = pd.DataFrame(dataset[picks])\n",
    "#     for column, typ in dataset.features.items():\n",
    "#         if isinstance(typ, datasets.ClassLabel):\n",
    "#             df[column] = df[column].transform(lambda i: typ.names[i])\n",
    "#     display(HTML(df.to_html()))\n",
    "    \n",
    "# show_random_elements(train_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize our Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=eval_dataset if training_args.do_eval else None,\n",
    "    compute_metrics=compute_metrics,\n",
    "    tokenizer=tokenizer,\n",
    "    # Data collator will default to DataCollatorWithPadding, so we change it if we already did the padding.\n",
    "    data_collator=default_data_collator if data_args.pad_to_max_length else None,\n",
    ")\n",
    "\n",
    "# Training\n",
    "if training_args.do_train:\n",
    "    trainer.train(\n",
    "        model_path=model_args.model_name_or_path if os.path.isdir(model_args.model_name_or_path) else None\n",
    "    )\n",
    "    trainer.save_model()  # Saves the tokenizer too for easy upload"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation\n",
    "eval_results = {}\n",
    "if training_args.do_eval:\n",
    "    logger.info(\"*** Evaluate ***\")\n",
    "\n",
    "    # Loop to handle MNLI double evaluation (matched, mis-matched)\n",
    "    tasks = [data_args.task_name]\n",
    "    eval_datasets = [eval_dataset]\n",
    "    if data_args.task_name == \"mnli\":\n",
    "        tasks.append(\"mnli-mm\")\n",
    "        eval_datasets.append(datasets[\"validation_mismatched\"])\n",
    "\n",
    "    for eval_dataset, task in zip(eval_datasets, tasks):\n",
    "        eval_result = trainer.evaluate(eval_dataset=eval_dataset)\n",
    "\n",
    "        output_eval_file = os.path.join(training_args.output_dir, f\"eval_results_{task}.txt\")\n",
    "        if trainer.is_world_process_zero():\n",
    "            with open(output_eval_file, \"w\") as writer:\n",
    "                logger.info(f\"***** Eval results {task} *****\")\n",
    "                for key, value in eval_result.items():\n",
    "                    logger.info(f\"  {key} = {value}\")\n",
    "                    writer.write(f\"{key} = {value}\\n\")\n",
    "\n",
    "        eval_results.update(eval_result)\n",
    "\n",
    "if training_args.do_predict:\n",
    "    logger.info(\"*** Test ***\")\n",
    "\n",
    "    # Loop to handle MNLI double evaluation (matched, mis-matched)\n",
    "    tasks = [data_args.task_name]\n",
    "    test_datasets = [test_dataset]\n",
    "    if data_args.task_name == \"mnli\":\n",
    "        tasks.append(\"mnli-mm\")\n",
    "        test_datasets.append(datasets[\"test_mismatched\"])\n",
    "\n",
    "    for test_dataset, task in zip(test_datasets, tasks):\n",
    "        # Removing the `label` columns because it contains -1 and Trainer won't like that.\n",
    "        test_dataset.remove_columns_(\"label\")\n",
    "        predictions = trainer.predict(test_dataset=test_dataset).predictions\n",
    "        predictions = np.squeeze(predictions) if is_regression else np.argmax(predictions, axis=1)\n",
    "\n",
    "        output_test_file = os.path.join(training_args.output_dir, f\"test_results_{task}.txt\")\n",
    "        if trainer.is_world_process_zero():\n",
    "            with open(output_test_file, \"w\") as writer:\n",
    "                logger.info(f\"***** Test results {task} *****\")\n",
    "                writer.write(\"index\\tprediction\\n\")\n",
    "                for index, item in enumerate(predictions):\n",
    "                    if is_regression:\n",
    "                        writer.write(f\"{index}\\t{item:3.3f}\\n\")\n",
    "                    else:\n",
    "                        item = label_list[item]\n",
    "                        writer.write(f\"{index}\\t{item}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The same over the folds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_folds_results = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for fold in range(10):\n",
    "    \n",
    "    args=['--model_name_or_path', 'bert-base-uncased',\n",
    "      '--train_file', f'eat_folded/eat_train_breakpoint_{fold}.csv',\n",
    "      '--validation_file', f'eat_folded/eat_test_breakpoint_{fold}.csv',\n",
    "      '--do_train',\n",
    "      '--do_eval',\n",
    "      '--max_seq_length', '256',\n",
    "       '--per_device_train_batch_size', '32',\n",
    "      '--learning_rate', '2e-5',\n",
    "       '--num_train_epochs', '5.0',\n",
    "       '--output_dir', './',\n",
    "       '--overwrite_output_dir']\n",
    "    \n",
    "    parser = HfArgumentParser((ModelArguments, DataTrainingArguments, TrainingArguments))\n",
    "\n",
    "    model_args, data_args, training_args = parser.parse_args_into_dataclasses(args)\n",
    "\n",
    "    if (\n",
    "        os.path.exists(training_args.output_dir)\n",
    "        and os.listdir(training_args.output_dir)\n",
    "        and training_args.do_train\n",
    "        and not training_args.overwrite_output_dir\n",
    "    ):\n",
    "        raise ValueError(\n",
    "            f\"Output directory ({training_args.output_dir}) already exists and is not empty. \"\n",
    "            \"Use --overwrite_output_dir to overcome.\"\n",
    "        )\n",
    "        \n",
    "    # Setup logging\n",
    "    logging.basicConfig(\n",
    "        format=\"%(asctime)s - %(levelname)s - %(name)s -   %(message)s\",\n",
    "        datefmt=\"%m/%d/%Y %H:%M:%S\",\n",
    "        level=logging.INFO if is_main_process(training_args.local_rank) else logging.WARN,\n",
    "    )\n",
    "\n",
    "    # Log on each process the small summary:\n",
    "    logger.warning(\n",
    "        f\"Process rank: {training_args.local_rank}, device: {training_args.device}, n_gpu: {training_args.n_gpu}\"\n",
    "        + f\"distributed training: {bool(training_args.local_rank != -1)}, 16-bits training: {training_args.fp16}\"\n",
    "    )\n",
    "    # Set the verbosity to info of the Transformers logger (on main process only):\n",
    "    if is_main_process(training_args.local_rank):\n",
    "        transformers.utils.logging.set_verbosity_info()\n",
    "    logger.info(f\"Training/evaluation parameters {training_args}\")\n",
    "    \n",
    "    # Set seed before initializing model.\n",
    "    set_seed(training_args.seed)\n",
    "    \n",
    "    # Get the datasets: you can either provide your own CSV/JSON training and evaluation files (see below)\n",
    "    # or specify a GLUE benchmark task (the dataset will be downloaded automatically from the datasets Hub).\n",
    "\n",
    "    # For CSV/JSON files, this script will use as labels the column called 'label' and as pair of sentences the\n",
    "    # sentences in columns called 'sentence1' and 'sentence2' if such column exists or the first two columns not named\n",
    "    # label if at least two columns are provided.\n",
    "    #\n",
    "    # If the CSVs/JSONs contain only one non-label column, the script does single sentence classification on this\n",
    "    # single column. You can easily tweak this behavior (see below)\n",
    "    #\n",
    "    # In distributed training, the load_dataset function guarantee that only one local process can concurrently\n",
    "    # download the dataset.\n",
    "    if data_args.task_name is not None:\n",
    "        # Downloading and loading a dataset from the hub.\n",
    "        datasets = load_dataset(\"glue\", data_args.task_name)\n",
    "    elif data_args.train_file.endswith(\".csv\"):\n",
    "        # Loading a dataset from local csv files\n",
    "        datasets = load_dataset(\n",
    "            \"csv\", data_files={\"train\": data_args.train_file, \"validation\": data_args.validation_file}\n",
    "        )\n",
    "    else:\n",
    "        # Loading a dataset from local json files\n",
    "        datasets = load_dataset(\n",
    "            \"json\", data_files={\"train\": data_args.train_file, \"validation\": data_args.validation_file}\n",
    "        )\n",
    "    # See more about loading any type of standard or custom dataset at\n",
    "    # https://huggingface.co/docs/datasets/loading_datasets.html.\n",
    "\n",
    "    # Labels\n",
    "    if data_args.task_name is not None:\n",
    "        is_regression = data_args.task_name == \"stsb\"\n",
    "        if not is_regression:\n",
    "            label_list = datasets[\"train\"].features[\"label\"].names\n",
    "            num_labels = len(label_list)\n",
    "        else:\n",
    "            num_labels = 1\n",
    "    else:\n",
    "        # Trying to have good defaults here, don't hesitate to tweak to your needs.\n",
    "        is_regression = datasets[\"train\"].features[\"label\"].dtype in [\"float32\", \"float64\"]\n",
    "        if is_regression:\n",
    "            num_labels = 1\n",
    "        else:\n",
    "            # A useful fast method:\n",
    "            # https://huggingface.co/docs/datasets/package_reference/main_classes.html#datasets.Dataset.unique\n",
    "            label_list = datasets[\"train\"].unique(\"label\")\n",
    "            label_list.sort()  # Let's sort it for determinism\n",
    "            num_labels = len(label_list)\n",
    "            \n",
    "    # Load pretrained model and tokenizer\n",
    "    #\n",
    "    # In distributed training, the .from_pretrained methods guarantee that only one local process can concurrently\n",
    "    # download model & vocab.\n",
    "    config = AutoConfig.from_pretrained(\n",
    "        model_args.config_name if model_args.config_name else model_args.model_name_or_path,\n",
    "        num_labels=num_labels,\n",
    "        finetuning_task=data_args.task_name,\n",
    "        cache_dir=model_args.cache_dir,\n",
    "    )\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\n",
    "        model_args.tokenizer_name if model_args.tokenizer_name else model_args.model_name_or_path,\n",
    "        cache_dir=model_args.cache_dir,\n",
    "        use_fast=model_args.use_fast_tokenizer,\n",
    "    )\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(\n",
    "        model_args.model_name_or_path,\n",
    "        from_tf=bool(\".ckpt\" in model_args.model_name_or_path),\n",
    "        config=config,\n",
    "        cache_dir=model_args.cache_dir,\n",
    "    )\n",
    "\n",
    "    # Preprocessing the datasets\n",
    "    if data_args.task_name is not None:\n",
    "        sentence1_key, sentence2_key = task_to_keys[data_args.task_name]\n",
    "    else:\n",
    "        # Again, we try to have some nice defaults but don't hesitate to tweak to your use case.\n",
    "        non_label_column_names = [name for name in datasets[\"train\"].column_names if name != \"label\"]\n",
    "        if \"sentence1\" in non_label_column_names and \"sentence2\" in non_label_column_names:\n",
    "            sentence1_key, sentence2_key = \"sentence1\", \"sentence2\"\n",
    "        else:\n",
    "            if len(non_label_column_names) >= 2:\n",
    "                sentence1_key, sentence2_key = non_label_column_names[:2]\n",
    "            else:\n",
    "                sentence1_key, sentence2_key = non_label_column_names[0], None\n",
    "\n",
    "    # Padding strategy\n",
    "    if data_args.pad_to_max_length:\n",
    "        padding = \"max_length\"\n",
    "        max_length = data_args.max_seq_length\n",
    "    else:\n",
    "        # We will pad later, dynamically at batch creation, to the max sequence length in each batch\n",
    "        padding = False\n",
    "        max_length = None\n",
    "\n",
    "    # Some models have set the order of the labels to use, so let's make sure we do use it.\n",
    "    label_to_id = None\n",
    "    if (\n",
    "        model.config.label2id != PretrainedConfig(num_labels=num_labels).label2id\n",
    "        and data_args.task_name is not None\n",
    "        and is_regression\n",
    "    ):\n",
    "        # Some have all caps in their config, some don't.\n",
    "        label_name_to_id = {k.lower(): v for k, v in model.config.label2id.items()}\n",
    "        if list(sorted(label_name_to_id.keys())) == list(sorted(label_list)):\n",
    "            label_to_id = {i: label_name_to_id[label_list[i]] for i in range(num_labels)}\n",
    "        else:\n",
    "            logger.warn(\n",
    "                \"Your model seems to have been trained with labels, but they don't match the dataset: \",\n",
    "                f\"model labels: {list(sorted(label_name_to_id.keys()))}, dataset labels: {list(sorted(label_list))}.\"\n",
    "                \"\\nIgnoring the model labels as a result.\",\n",
    "            )\n",
    "    elif data_args.task_name is None:\n",
    "        label_to_id = {v: i for i, v in enumerate(label_list)}\n",
    "\n",
    "    def preprocess_function(examples):\n",
    "        # Tokenize the texts\n",
    "        args = (\n",
    "            (examples[sentence1_key],) if sentence2_key is None else (examples[sentence1_key], examples[sentence2_key])\n",
    "        )\n",
    "        result = tokenizer(*args, padding=padding, max_length=max_length, truncation=True)\n",
    "\n",
    "        # Map labels to IDs (not necessary for GLUE tasks)\n",
    "        if label_to_id is not None and \"label\" in examples:\n",
    "            result[\"label\"] = [label_to_id[l] for l in examples[\"label\"]]\n",
    "        return result\n",
    "\n",
    "    datasets = datasets.map(preprocess_function, batched=True, load_from_cache_file=not data_args.overwrite_cache)\n",
    "\n",
    "    train_dataset = datasets[\"train\"]\n",
    "    eval_dataset = datasets[\"validation_matched\" if data_args.task_name == \"mnli\" else \"validation\"]\n",
    "    if data_args.task_name is not None:\n",
    "        test_dataset = datasets[\"test_matched\" if data_args.task_name == \"mnli\" else \"test\"]\n",
    "\n",
    "    # Log a few random samples from the training set:\n",
    "    for index in random.sample(range(len(train_dataset)), 3):\n",
    "        logger.info(f\"Sample {index} of the training set: {train_dataset[index]}.\")\n",
    "\n",
    "    # Get the metric function\n",
    "    if data_args.task_name is not None:\n",
    "        metric = load_metric(\"glue\", data_args.task_name)\n",
    "    # TODO: When datasets metrics include regular accuracy, make an else here and remove special branch from\n",
    "    # compute_metrics\n",
    "\n",
    "    # You can define your custom compute_metrics function. It takes an `EvalPrediction` object (a namedtuple with a\n",
    "    # predictions and label_ids field) and has to return a dictionary string to float.\n",
    "#     def compute_metrics(p: EvalPrediction):\n",
    "#         preds = p.predictions[0] if isinstance(p.predictions, tuple) else p.predictions\n",
    "#         preds = np.squeeze(preds) if is_regression else np.argmax(preds, axis=1)\n",
    "#         if data_args.task_name is not None:\n",
    "#             result = metric.compute(predictions=preds, references=p.label_ids)\n",
    "#             if len(result) > 1:\n",
    "#                 result[\"combined_score\"] = np.mean(list(result.values())).item()\n",
    "#             return result\n",
    "#         elif is_regression:\n",
    "#             return {\"mse\": ((preds - p.label_ids) ** 2).mean().item()}\n",
    "#         else:\n",
    "#             return {\"accuracy\": (preds == p.label_ids).astype(np.float32).mean().item()}\n",
    "        \n",
    "    def compute_metrics(pred: EvalPrediction):\n",
    "        labels = pred.label_ids\n",
    "        preds = pred.predictions.argmax(-1)\n",
    "        precision, recall, f1, _ = precision_recall_fscore_support(labels, preds, average='macro')\n",
    "        acc = accuracy_score(labels, preds)\n",
    "        return {\n",
    "            'accuracy': acc,\n",
    "            'f1': f1,\n",
    "            'precision': precision,\n",
    "            'recall': recall\n",
    "        }\n",
    "\n",
    "    ####################### TRAIN ######################\n",
    "    # Initialize our Trainer\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=train_dataset,\n",
    "        eval_dataset=eval_dataset if training_args.do_eval else None,\n",
    "        compute_metrics=compute_metrics,\n",
    "        tokenizer=tokenizer,\n",
    "        # Data collator will default to DataCollatorWithPadding, so we change it if we already did the padding.\n",
    "        data_collator=default_data_collator if data_args.pad_to_max_length else None,\n",
    "    )\n",
    "\n",
    "    # Training\n",
    "    if training_args.do_train:\n",
    "        trainer.train(\n",
    "            model_path=model_args.model_name_or_path if os.path.isdir(model_args.model_name_or_path) else None\n",
    "        )\n",
    "        trainer.save_model()  # Saves the tokenizer too for easy upload\n",
    "        \n",
    "        \n",
    "    # Evaluation\n",
    "    eval_results = {}\n",
    "    if training_args.do_eval:\n",
    "        logger.info(\"*** Evaluate ***\")\n",
    "\n",
    "        # Loop to handle MNLI double evaluation (matched, mis-matched)\n",
    "        tasks = [data_args.task_name]\n",
    "        eval_datasets = [eval_dataset]\n",
    "        if data_args.task_name == \"mnli\":\n",
    "            tasks.append(\"mnli-mm\")\n",
    "            eval_datasets.append(datasets[\"validation_mismatched\"])\n",
    "\n",
    "        for eval_dataset, task in zip(eval_datasets, tasks):\n",
    "            eval_result = trainer.evaluate(eval_dataset=eval_dataset)\n",
    "\n",
    "            output_eval_file = os.path.join(training_args.output_dir, f\"eval_results_{task}.txt\")\n",
    "            if trainer.is_world_process_zero():\n",
    "                with open(output_eval_file, \"w\") as writer:\n",
    "                    logger.info(f\"***** Eval results {task} *****\")\n",
    "                    for key, value in eval_result.items():\n",
    "                        logger.info(f\"  {key} = {value}\")\n",
    "                        writer.write(f\"{key} = {value}\\n\")\n",
    "\n",
    "            eval_results.update(eval_result)\n",
    "            \n",
    "    all_folds_results.append(eval_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f1s = [i['eval_f1'] for i in all_folds_results if 'eval_f1' in i.keys()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean(f1s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.std(f1s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean([d['eval_accuracy'] for d in all_folds_results])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.std([d['eval_accuracy'] for d in all_folds_results])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "metric = load_metric(\"glue\", data_args.task_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_metric()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "eecs595",
   "language": "python",
   "name": "eecs595"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
